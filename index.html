<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="小二郎">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="小二郎">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="chen shuai">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>小二郎</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">小二郎</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/20/spark-dataframe%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chen shuai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小二郎">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/20/spark-dataframe%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/" itemprop="url">spark dataframe常用操作</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-20T20:00:42+08:00">
                2020-06-20
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/15/vivo-hr%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chen shuai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小二郎">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/15/vivo-hr%E9%9D%A2%E8%AF%95%E8%AE%B0%E5%BD%95/" itemprop="url">vivo-hr面试记录</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-15T14:15:10+08:00">
                2020-06-15
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <ol>
<li>自我介绍</li>
<li>你的成绩</li>
<li>奖学金</li>
<li>是否有专利</li>
<li>是否有论文</li>
<li>为了这次面试你准备了什么</li>
<li>你觉得你相比其他人应聘者你的优势在哪里</li>
<li>你相比其他的应聘者你的劣势在哪里</li>
<li>你遇到的最困难的一件事</li>
<li>你性格的优点</li>
<li>你性格的缺点</li>
<li>如果你自己在忙，同事来找你帮忙你会怎么做</li>
<li>vivo一款手机销量不好，你会从哪些角度分析</li>
<li>为什么选择vivo</li>
<li>有什么事是你一直在坚持的</li>
<li>家庭情况</li>
<li>职业规划</li>
<li>期望工作地</li>
<li>期望薪资</li>
</ol>
<p>总的来说优点很好答，一扯到缺点hr就会朝工作中带，给你埋坑。。。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/14/%E9%9D%A2%E8%AF%95%E4%B9%8B%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chen shuai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小二郎">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/14/%E9%9D%A2%E8%AF%95%E4%B9%8B%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%9F%BA%E7%A1%80/" itemprop="url">面试之计算机基础</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-14T15:27:51+08:00">
                2020-06-14
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="计算机基础"><a href="#计算机基础" class="headerlink" title="计算机基础"></a>计算机基础</h2><h3 id="计算机网络基础"><a href="#计算机网络基础" class="headerlink" title="计算机网络基础"></a>计算机网络基础</h3><p>五层协议：物理层、数据链路层、网络层、传输层、应用层</p>
<p>数据往下传递要加入下层协议所需的首部或尾部，向上传递要拆开</p>
<p>传输层包含两种协议：TCP和UDP，主要是为进程提供通用的数据传输服务</p>
<p>应用层为特定应用程序提供数据传输服务，比如HTPP，FTP协议</p>
<p>TCP和UDP</p>
<p>1、TCP面向连接（如打电话要先拨号建立连接）;UDP是无连接的，即发送数据之前不需要建立连接<br>2、TCP提供完整性服务。也就是说，通过TCP连接传送的数据，无差错，不丢失，不重复，且按序到达;UDP提供及时性服务，尽最大努力交付，即不保证可靠交付</p>
<p>TCP三次握手，假设A为客户端，B为服务器端</p>
<ul>
<li>B要一直处于监听状态，等待客户端连接请求</li>
<li>A向B发送连接请求报文，并选择一个初始序号x</li>
<li>B收到后，如果同意链接，给出一个确认号x+1,并选择一个初始序号y</li>
<li>A收到确认后，还要再次发出确认报文，确认号y+1,序号x+1</li>
<li>B收到后，连接建立</li>
</ul>
<p>第三次握手是为了防止无效的连接请求到达服务器，然后错误的建立连接。</p>
<p>TCP四次挥手，A为客户端，B为服务器端</p>
<ul>
<li>A发送连接释放报文</li>
<li>B收到后发出确认，但此时TCP是半关闭状态，只能B向A发送数据</li>
<li>当B不需要连接时，发送连接释放报文</li>
<li>A收到后发出确认，等待一段时间后释放连接</li>
<li>B收到确认后释放连接</li>
</ul>
<h3 id="linux常用命令"><a href="#linux常用命令" class="headerlink" title="linux常用命令"></a>linux常用命令</h3><p>nohup &amp;, nohup表示不间断运行，但是无法输入输出；&amp;表示在后台运行</p>
<p>find / -name file1 从 ‘/‘ 开始进入根文件系统搜索文件和目录 </p>
<p>df -h 显示已经挂载的分区列表 </p>
<p>du -sh dir1 估算目录 ‘dir1’ 已经使用的磁盘空间’ </p>
<p>head -2 file1 查看一个文件的前两行 </p>
<p>Kill -9 强制杀死进程 -9表示强制</p>
<p>lsof -i lsof表示list open file，-i表示端口 进程号什么的</p>
<p>ps -aux | grep Jupyter  ps显示进程 a是所有进程 u是指定用户为主的格式 ，grep过滤</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">原字符串：将文件名需要替换的字符串；</span><br><span class="line">目标字符串：将文件名中含有的原字符替换成目标字符串；</span><br><span class="line">文件：指定要改变文件名的文件列表。</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rename main1.c main.c main1.c</span><br></pre></td></tr></table></figure>

<h3 id="git常用命令"><a href="#git常用命令" class="headerlink" title="git常用命令"></a>git常用命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">在当前目录新建一个Git代码库</span><br><span class="line">$ git init</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">添加当前目录的所有文件到暂存区</span><br><span class="line">$ git add .</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">提交暂存区到仓库区</span><br><span class="line">$ git commit -m [message]</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">列出所有本地分支</span><br><span class="line">$ git branch</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">切换到指定分支，并更新工作区</span><br><span class="line">$ git checkout [branch-name]</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">显示暂存区和工作区的代码差异</span><br><span class="line">$ git diff</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">取回远程仓库的变化，并与本地分支合并</span><br><span class="line">$ git pull [remote] [branch]</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">上传本地指定分支到远程仓库</span><br><span class="line">$ git push [remote] [branch]</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 暂时将未提交的变化移除，稍后再移入</span></span><br><span class="line">$ git stash</span><br></pre></td></tr></table></figure>

<h3 id="计算机操作系统"><a href="#计算机操作系统" class="headerlink" title="计算机操作系统"></a>计算机操作系统</h3><p>进程和线程区别？</p>
<p>线程是cpu独立调度和分配的基本单位，进程是资源分配的基本单位，但是线程不拥有资源，线程可以访问隶属进程的资源。进程可以看作是一个程序，一个程序可以运行多个子任务，这个子任务就是线程</p>
<p>通信方面：线程间可以通过直接读写同一进程中的数据进行通信，但是进程通信需要借助 IPC。</p>
<h4 id="子进程、父进程、僵尸进程、孤儿进程"><a href="#子进程、父进程、僵尸进程、孤儿进程" class="headerlink" title="子进程、父进程、僵尸进程、孤儿进程"></a>子进程、父进程、僵尸进程、孤儿进程</h4><p>通过fork创建进程的这个进程是父进程，被创建的这个进程就是子进程</p>
<p>一般来说子进程运行结束后会发一个信号给父进程，父进程收到后会通知系统清理掉子进程的内存，但是如果没这么做，子进程一直存在成了僵尸进程</p>
<p>孤儿进程就是父进程结束后还在运行的子进程</p>
<h3 id="python语言基础"><a href="#python语言基础" class="headerlink" title="python语言基础"></a>python语言基础</h3><h4 id="多线程、多进程"><a href="#多线程、多进程" class="headerlink" title="多线程、多进程"></a>多线程、多进程</h4><p>python由于GIL的存在，只能运行一个线程。适合爬虫这种io密集型的任务</p>
<p>要并行的话，使用multiprocessing模块，适合计算密集型的任务</p>
<h4 id="浅拷贝和深拷贝"><a href="#浅拷贝和深拷贝" class="headerlink" title="浅拷贝和深拷贝"></a>浅拷贝和深拷贝</h4><p>浅拷贝是拷贝父对象，但是子对象没有拷贝下来，比如列表里的元素有一个是列表，那么里面的列表是子对象，拷贝下来更改子对象，原数据的子对象也会跟着更改</p>
<p>深拷贝就是全拷贝下来</p>
<h4 id="迭代器和生成器"><a href="#迭代器和生成器" class="headerlink" title="迭代器和生成器"></a>迭代器和生成器</h4><p>支持迭代器协议就是实现对象的<strong>iter</strong>()和next()方法。其中<strong>iter</strong>()方法返回迭代器对象本身；next()方法返回容器的下一个元素，在结尾时引发StopIteration异常。</p>
<p>生成器使用yield关键字实现，这个就相当于是一个return 不过每次运行到这return后，下一次在调用会继续接着上次的状态运行</p>
<h4 id="python字典实现"><a href="#python字典实现" class="headerlink" title="python字典实现"></a>python字典实现</h4><p>内部通过哈希表实现，哈希表就是利用哈希函数将key映射成一个整数，代表value的下标索引</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/13/tensorflow%E5%A4%9A%E7%BB%B4%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chen shuai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小二郎">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/13/tensorflow%E5%A4%9A%E7%BB%B4%E7%9F%A9%E9%98%B5%E7%9B%B8%E4%B9%98/" itemprop="url">tensorflow多维矩阵相乘</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-13T17:46:41+08:00">
                2020-06-13
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>矩阵乘法一般都是针对的二维矩阵，但是在实际应用中有时会遇到多维矩阵相乘的情况，比如计算Attention的时候，那么tensorflow内部是如何处理多维矩阵相乘的呢？</p>
<p>首先看一个示例：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfqt31vmtlj308p04xt8t.jpg" alt="image-20200613174832284"></p>
<p>在tensorflow内部针对多维矩阵相乘，进行的都是batch_mat_mul，matmul针对的是每个batch的切片，也就还是二维矩阵相乘。所以进行多维矩阵相乘的时候，要保证除了最后两维，其他的维度要一一对应相等。最后两个维度要满足数学上矩阵相乘的要求。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/09/NLP%E4%B8%AD%E7%9A%84%E6%96%87%E6%9C%AC%E5%A2%9E%E5%BC%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chen shuai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小二郎">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/09/NLP%E4%B8%AD%E7%9A%84%E6%96%87%E6%9C%AC%E5%A2%9E%E5%BC%BA/" itemprop="url">NLP中的文本增强</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-09T14:58:29+08:00">
                2020-06-09
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>实际应用中，有时候会遇到数据集数量过少的情况，对此可以采用文本增强的方法，增加数据集的数量，从而提高后续的模型性能。</p>
<h2 id="词语替换"><a href="#词语替换" class="headerlink" title="词语替换"></a>词语替换</h2><p>对特定的词语或者短语进行同义词替换，可使用同义词典替换</p>
<h2 id="噪音注入"><a href="#噪音注入" class="headerlink" title="噪音注入"></a>噪音注入</h2><p>随机删除某些词，随机调换某些词的顺序</p>
<h2 id="回译"><a href="#回译" class="headerlink" title="回译"></a>回译</h2><p>将中文翻译成英文再翻译回中文</p>
<h2 id="半监督方法"><a href="#半监督方法" class="headerlink" title="半监督方法"></a>半监督方法</h2><p>半监督学习是一种同时利用有标签数据和无标签数据进行学习的一种方法</p>
<h3 id="Pseudo-label（伪标签）"><a href="#Pseudo-label（伪标签）" class="headerlink" title="Pseudo label（伪标签）"></a>Pseudo label（伪标签）</h3><ul>
<li>将有标签部分数据分为两份：train_set&amp;validation_set，并训练出最优的model1</li>
<li>用model1对未知标签数据(test_set)进行预测，给出伪标签结果pseudo-labeled</li>
<li>将train_set中抽取一部分做新的validation_set，把剩余部分与pseudo-labeled部分融合作为新的train_set，训练出最优的model2</li>
<li>再用model2对未知标签数据(test_set)进行预测，得到最终的final result label</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/08/SQL%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chen shuai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小二郎">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/08/SQL%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5/" itemprop="url">SQL基本概念</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-08T14:52:48+08:00">
                2020-06-08
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="主键"><a href="#主键" class="headerlink" title="主键"></a>主键</h1><p>主键就是一个一个能够区分不同记录的字段，选取的基本原则是：不使用任何业务相关的字段作为主键。</p>
<p>实际开发中，通常使用自增整数型的主键，一般命名为id。</p>
<p>联合主键就是联合几个字断作为主键，通常不使用。</p>
<h1 id="外键"><a href="#外键" class="headerlink" title="外键"></a>外键</h1><p>一张表里通过某个字断与另一张表建立关系，这个字断叫做外键。要通过定义外键约束实现。这样在插入的时候，要保证这个外键也是合法有效的。</p>
<p>不过实际开发中，外键使用的貌似也不多。</p>
<h1 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h1><p>索引是关系数据库中对某一列或多个列的值进行预排序的数据结构。通过使用索引，可以让数据库系统不必扫描整个表，而是直接定位到符合条件的记录，这样就大大加快了查询速度。</p>
<p>比如对学生表里学生的成绩得分创建索引。</p>
<p>可以对一张表创建多个索引。索引的优点是提高了查询效率，缺点是在插入、更新和删除记录时，需要同时修改索引，因此，索引越多，插入、更新和删除记录的速度就越慢。</p>
<p>对于主键，关系数据库会自动对其创建主键索引。使用主键索引的效率是最高的，因为主键会保证绝对唯一。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chen shuai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小二郎">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" itemprop="url">深度学习常见问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-07T12:50:33+08:00">
                2020-06-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>梯度是一个函数变化最快的方向，梯度是一个向量，沿着梯度的正方向，可以最快的达到最大值；沿着梯度的反方向，可以最快的达到最小值。</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcr4000619j30bm036glm.jpg" alt="image-20200312140512887"></p>
<p>L1正则化不可导怎么处理，使用坐标轴下降法，固定其他参数，保留一个，然后求极值</p>
<p>max操作不可导，可以分段处理</p>
<p>Hinge loss也可以分段处理</p>
<h4 id="SGD、BGD、MBGD"><a href="#SGD、BGD、MBGD" class="headerlink" title="SGD、BGD、MBGD"></a>SGD、BGD、MBGD</h4><p>SGD就是每次只计算一个样本的梯度来更新参数值</p>
<p>BGD就是使用所有的样本的梯度平均值来更新参数值</p>
<p>MBGD就是采用部分样本的梯度平均值来更新参数值</p>
<p>按理说是计算每个样本的梯度，然后取均值当作最后该权值的梯度，实际操作中是直接对loss加和再取平均，是一样的效果。</p>
<h4 id="梯度下降的优化版本"><a href="#梯度下降的优化版本" class="headerlink" title="梯度下降的优化版本"></a>梯度下降的优化版本</h4><p>我们知道如果每次都是对一个batch计算梯度来更新，很容易造成抖动的情况，所以有没有办法能够稳定梯度，也就是不仅考虑当前的梯度还要考虑之前的梯度</p>
<h5 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h5><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcr4swg8psj30j607wjry.jpg" alt="image-20200312143259002" style="zoom:33%;" />

<p>带冲量的梯度下降。它的动机就是说保存历史累计梯度，其实就是对历史累计梯度做了一个加权和，称作冲量。然后计算当前时刻的梯度，如果与冲量方向一致，那么就是叠加进去，如果相反，冲量也会减少。这种优化器的好处也显而易见，考虑了历史累计梯度，可以避免模型在某个时刻过于震荡，从而加速模型的训练。</p>
<h5 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h5><p>是RMSProp前一个版本，让学习率衰减，让学习率除以历史累积的梯度平方和的开根</p>
<h5 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h5><p>梯度的问题解决了，那么学习率是不是有更好的办法让它能够自适应的改变呢</p>
<p>是Adagrad改进版，adagrad训练到一定程度，学习率太小了，不太靠谱。所以采用指数衰减的策略，只考虑最近的梯度</p>
<p>可以看到，s是关于梯度的，如果梯度太大，那么也就是s很大，用alpha去除，那么alpha就会降低，相当于防止过度抖动</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcr57d8xj8j315c08at9p.jpg" alt="image-20200312144653595"></p>
<h5 id="Adam算法"><a href="#Adam算法" class="headerlink" title="Adam算法"></a>Adam算法</h5><p>Adam算法就是取两者的优点，既平滑梯度，也平滑学习率</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcr59qsgr2j31ns0a2jt9.jpg" alt="image-20200312144910210"></p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>反向传播就是利用链式求导法则一步步后推罢了</p>
<h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcr77g5w6wj31b80jetfk.jpg" alt="image-20200312155609581"></p>
<h4 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h4><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcrfit1wecj30u00ufgx2.jpg" alt="image-20200312204349392"></p>
<h4 id="pooling层反向传播"><a href="#pooling层反向传播" class="headerlink" title="pooling层反向传播"></a>pooling层反向传播</h4><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdnkiww5zjj30yr0u0b2b.jpg" alt="image-20200409155445667"></p>
<h4 id="1-1卷积有什么用"><a href="#1-1卷积有什么用" class="headerlink" title="1*1卷积有什么用"></a>1*1卷积有什么用</h4><p>一方面呢实现了对数据通道数的一个降纬或升纬。另一方面呢可以对多个通道feature map的信息做一个融合。实现了全连接的效果</p>
<h4 id="全连接和CNN区别"><a href="#全连接和CNN区别" class="headerlink" title="全连接和CNN区别"></a>全连接和CNN区别</h4><p>全连接的话是对所有的输入元素做一个加权和，有种采用大的卷积核的感觉</p>
<p>而CNN是采用小的卷积核，然后权值共享，通过滑动卷积核来实现对整个feature map的特征提取</p>
<h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><h4 id="RNN展开图"><a href="#RNN展开图" class="headerlink" title="RNN展开图"></a>RNN展开图</h4><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcsn3w3peyj31fe0fwjy5.jpg" alt="image-20200313215148276"></p>
<h4 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h4><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcsn4ag86mj30p607q78a.jpg" alt="image-20200313215214682" style="zoom:33%;" />

<h4 id="反向传播BPTT算法"><a href="#反向传播BPTT算法" class="headerlink" title="反向传播BPTT算法"></a>反向传播BPTT算法</h4><p>BPTT其实就是标准的梯度反向传播算法在RNN中的应用罢了，只是名字搞得花里胡哨，因为S状态是传递的，而且与W、U有关，所以导致求导的时候也是要按时间步展开的，具体流程流程如下</p>
<p>设E3为第三个时间步的损失，下面开始求偏导</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcsn8so0iyj30yz0u0dxa.jpg" alt="image-20200313215634543"></p>
<h4 id="梯度消失、爆炸原因"><a href="#梯度消失、爆炸原因" class="headerlink" title="梯度消失、爆炸原因"></a>梯度消失、爆炸原因</h4><p>上面这个公式还要在转换一下形式</p>
<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcsn9zwmu4j30o004yaar.jpg" alt="image-20200313215743977" style="zoom:33%;" />

<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcsnb00sslj30w207umy9.jpg" alt="image-20200313215842073" style="zoom:50%;" />

<p>可以看出这里对tanh函数的导数有一个长范围的连乘，但是tanh导数的值在0-1之间，且大多数时候都是小于1的，所以就很容易造成梯度消失。同理如果那个矩阵中有值比较大，则连乘后就容易造成梯度爆炸。</p>
<h4 id="LSTM网络"><a href="#LSTM网络" class="headerlink" title="LSTM网络"></a>LSTM网络</h4><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gct9zg72jpj31hu0j8jx6.jpg" alt="image-20200314110321695"></p>
<p>完整推导</p>
<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gctbic4luzj30rg1947wi.jpg" alt="image-20200314115606081" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gctbj90ok3j30ii0bgtja.jpg" alt="image-20200314115659042" style="zoom: 50%;" />

<h5 id="参数量计算"><a href="#参数量计算" class="headerlink" title="参数量计算"></a>参数量计算</h5><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdby46nrlqj310403ojrn.jpg" alt="image-20200330143826086"></p>
<p>三个门的计算就是3个，还要在加一个c_t的计算</p>
<h4 id="GRU网络"><a href="#GRU网络" class="headerlink" title="GRU网络"></a>GRU网络</h4><p>GRU就是将LSTM的三个门变成了两个门，将LSTM的输入门和遗忘门合并为update门，还有一个reset门。由于在LSTM中隐状态ht也是从Ct输出转化得到的，那么干脆把Ct当作ht，然后取消了ht。</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gctbkuevq0j319i0g2gpa.jpg" alt="image-20200314115831618"></p>
<h4 id="RNN、LSTM、GRU对比"><a href="#RNN、LSTM、GRU对比" class="headerlink" title="RNN、LSTM、GRU对比"></a>RNN、LSTM、GRU对比</h4><p>首先这三种模型都是循环神经网络，LSTM、GRU是RNN的变种，适用于序列建模</p>
<p>与普通的全联接模型相比呢，循环神经网络的特点就是当前的输出不仅与当前的输入x有关，还和上一个时刻的隐状态h有关。</p>
<p>但是RNN在实际应用中有一个问题就是容易出现梯度消失或者爆炸，这个怎么理解呢，可以误差反向传播公示推倒一遍</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1geju9cxen8j30u0156qv7.jpg" alt="image-20200507134940225"></p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>dropout的核心思想就是通过使得部分神经元的值为0，从而相当于每次只是训练一个子网络，那么当测试的时候就是很多个子网络ensemble的结果，有点像是bagging的思想</p>
<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gctf7fym54j31140rkk2o.jpg" alt="image-20200314140402032" style="zoom:33%;" />

<p>公式可以看出就是一一个概率值决定哪些神经元为0，不过有一点要注意，每次dropout后，要对该层的输出进行rescale，因为你丢掉了部分值，为了测试的时候不影响，所以你得除以kepp_prob p，这样的话保证这层的值是差不多的。</p>
<h4 id="dropout为什么可以缓解过拟合"><a href="#dropout为什么可以缓解过拟合" class="headerlink" title="dropout为什么可以缓解过拟合"></a>dropout为什么可以缓解过拟合</h4><ol>
<li>通过每次随机使得一部分神经元失活，相当于给模型添加噪声，强制模型学习一些更鲁棒的特征</li>
<li>另一方面使得网络结构更简单些，模型简单从而使得解空间变小，也可以起到缓解过拟合的目的</li>
<li>最终输出相当于做了一个bagging</li>
</ol>
<h4 id="keras中dropout代码实现"><a href="#keras中dropout代码实现" class="headerlink" title="keras中dropout代码实现"></a>keras中dropout代码实现</h4><p>可以看出keras中对dropout就是每次dropout后进行一个rescale。</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gctfb9wg08j317e0f2gw2.jpg" alt="image-20200314140743638"></p>
<h4 id="dropout缺点"><a href="#dropout缺点" class="headerlink" title="dropout缺点"></a>dropout缺点</h4><ol>
<li>因为每次都是相当于训练一个子网络，所以收敛会变慢，训练时间会变长</li>
<li>代价函数没法被准确定义，因为每次都随机置0一些神经元</li>
</ol>
<h4 id="dropout反向传播"><a href="#dropout反向传播" class="headerlink" title="dropout反向传播"></a>dropout反向传播</h4><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjmmbmbvoj30tv0h0ac3.jpg" alt="image-20200607124549136"></p>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><p>作者使用三角函数编码和学习编码效果差不多，所以为了简单起见，直接使用了三角函数编码</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd3znzwij3j31a40dcwh4.jpg" alt="image-20200323172721437"></p>
<h4 id="位置编码为何有效"><a href="#位置编码为何有效" class="headerlink" title="位置编码为何有效"></a>位置编码为何有效</h4><p>论文里面说法是sin cos函数可以引入相关位置信息，就是pos+k的位置编码可以通过pos的编码加一个线性函数表示出来</p>
<h4 id="相对位置编码"><a href="#相对位置编码" class="headerlink" title="相对位置编码"></a>相对位置编码</h4><p>对原论文中的相似度计算公式展开如下，由于在第四个部分引入了两个参数，很容易导致相对位置信息的丢失。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfkqj79awrj30lr076ac3.jpg" alt="image-20200608114651953"></p>
<p>所以原班人马对此进行改进，针对相似度计算公式进行的改进，其实就是引入了两个可训练的参数，便于还原相对位置信息</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfkqlame51j308d046wei.jpg" alt="image-20200608114855607"></p>
<p>后来transformer-xl也提出了一个新的相对位置编码，也是对相似度计算进行的一个改写。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfkr1rc9p7j30kf06a778.jpg" alt="image-20200608120442232"></p>
<h4 id="MultiHead-Attention"><a href="#MultiHead-Attention" class="headerlink" title="MultiHead Attention"></a>MultiHead Attention</h4><p>首先要意识到multihead attention其实就是self-attention，只不过是切分成几个头然后在拼接的</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd3zskibs8j313g0dgtbf.jpg" alt="image-20200323173148012"></p>
<p>然后这里的self-attention计算方式 其实采用的是scaled dot-production</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd3zx4w0e7j30rc06wdgg.jpg" alt="image-20200323173611197"></p>
<p>为什么使用scaled呢</p>
<p>直观来说，dk维度较大时，激活值趋紧于0或1，导致这层的梯度较小，不能很好的反向传播</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd3zyjt2r0j31gu04ywg0.jpg" alt="image-20200323173732423"></p>
<h5 id="softmax求导"><a href="#softmax求导" class="headerlink" title="softmax求导"></a>softmax求导</h5><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfktxuudprj30ti0eydhs.jpg" alt="image-20200608134445286"></p>
<h4 id="为什么使用MultiHead-Attention"><a href="#为什么使用MultiHead-Attention" class="headerlink" title="为什么使用MultiHead Attention"></a>为什么使用MultiHead Attention</h4><p>作者的解释是不同的头可以关注不同的表示空间，这样的话多个头就可以关注多个不同的特征，最后在拼接，这样的话无疑能捕捉到更多的句子信息</p>
<h4 id="为什么使用layer-normalization而不是batch-normalization"><a href="#为什么使用layer-normalization而不是batch-normalization" class="headerlink" title="为什么使用layer_normalization而不是batch_normalization"></a>为什么使用layer_normalization而不是batch_normalization</h4><p>Batch_normalization简介：</p>
<p>BN主要是针对batch这一维度上的值做一个规范化，</p>
<p>从两个角度来思考BN：</p>
<ol>
<li>ICS 内部分布偏移现象，原论文的解释，针对某一层，随着参数的改变，输出的分布也是不断改变的，模型学习的就是数据的分布，这样变化的太频繁不利于收敛，所以作归一化，保证输出稳定。另外为了能还原分布，还额外引入变量，模型可以自己学习。</li>
<li>sigmoid做激活函数的话，激活函数的敏感区域比较小，如果上一层输出值比较分散，会造成梯度较小，所以用BN将btach这个维度的分布转换成一个正态分布。但是为了转换后的分布还是要和以前的分布能够还原，所以还要再做个转换</li>
</ol>
<p>Layer_normalization的优势：</p>
<ul>
<li>BN是对一个batch的统计量，所以batch过小，那么得到的统计值不准确。而LN是针对一个样本去统计，所以不存在这个问题</li>
<li>BN应用在RNN中时，由于有多个time_step，所以要每个time_step都要统计，这就需要额外的开销。LN没有这个问题</li>
<li>一篇论文的说法，在transformer训练时，batch维度上的值波动比较大，如果使用BN会导致模型的不稳定</li>
</ul>
<p>BN和LN推理阶段怎么处理？</p>
<p>LN好理解，是针对样本的，所以测试的时候直接算就完事了</p>
<p>BN麻烦点，需要用到历史累计的方差，均值的滑动平均</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gear2406dej31740iwwgn.jpg" alt="image-20200429170903485"></p>
<h4 id="self-attention和feed-forward层作用"><a href="#self-attention和feed-forward层作用" class="headerlink" title="self-attention和feed_forward层作用"></a>self-attention和feed_forward层作用</h4><p>self-attention就是自身的每个词去和其他词做一个attention计算，然后转化为其他所有词的一个加权和，这样的话每个词就和其他词建立了联系，相当于学习到了一个上下文特征。multi-head attention就是不同的头去捕捉不同的特征，这样提取特征的能力就更强。</p>
<p>Feed-forward的作用我自己的理解就是multi-head attention后，模型需要对层的输出做一个信息整合，所以用两层全联接加非线性激活函数来做一个信息融合</p>
<h4 id="Encoder-decoder间的attention"><a href="#Encoder-decoder间的attention" class="headerlink" title="Encoder decoder间的attention"></a>Encoder decoder间的attention</h4><p>K,V来自于编码器的输出，Q来自于上一时刻解码器的输出</p>
<h4 id="为什么PE相加不是concate"><a href="#为什么PE相加不是concate" class="headerlink" title="为什么PE相加不是concate"></a>为什么PE相加不是concate</h4><p>我的想法就是这层组合的embedding进入下一层网络时还是要经过全连接进行转换的，这里如果拼接的话那么还是相当于加权和的形式，和直接加和在过全连接差不多。所以说两种方式应该效果上是差不多的。</p>
<h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p>BERT是一个双向语言模型，主要包含两个阶段，分别是预训练语言模型和下游任务fine-tune</p>
<p>预训练语言模型阶段主要是采用遮蔽语言模型的训练方法，也就是将一句话15%的token置换成[MASK]，但担心下游任务不会有[MASK]标志符，所以又对这些[MASK]进一步处理。最后模型的目标就是通过上下文去预测被mask掉的这个token。另外为了捕捉句子间的关系，还设计了一个NSP任务，连续的两句话构成正例，否则构成负例。</p>
<p>针对下游任务的fine-tune呢，就是将最后的输出层第一个token的表示拿出来作为输入句子的表示，后接分类层做文本分类、情感分析等任务</p>
<h4 id="为什么采用warm-up"><a href="#为什么采用warm-up" class="headerlink" title="为什么采用warm-up"></a>为什么采用warm-up</h4><p>直观上的理解：一开始loss比较大，如果学习率过大的话，会造成模型参数的巨大抖动，所以先慢慢来，逐步提高</p>
<p>这样就是有助于保证模型的稳定</p>
<p>学习率后期的decay也是怕学习率过大导致震荡，所以逐步减小</p>
<h4 id="位置编码-1"><a href="#位置编码-1" class="headerlink" title="位置编码"></a>位置编码</h4><p>BERT采用学习的位置编码，而不是transformer所采用的三角函数</p>
<h4 id="BERT之后和之前的工作"><a href="#BERT之后和之前的工作" class="headerlink" title="BERT之后和之前的工作"></a>BERT之后和之前的工作</h4><p>BERT之后的工作主要集中在两方面，第一种是使BERT更轻量化，第二种是更改或者优化预训练任务使得性能更好</p>
<h5 id="轻量化"><a href="#轻量化" class="headerlink" title="轻量化"></a>轻量化</h5><p>典型工作，ALBERT</p>
<p>主要集中在以下几点：1. Word embedding 分解，降低word embedding，加大hidden size；vocab*e , 减小e. 2. 跨层参数共享，共享feed_forward网络，共享attention那一层参数 3. 取消NSP任务，提出句子顺序是否被调换任务，即SOP</p>
<h5 id="更改预训练任务"><a href="#更改预训练任务" class="headerlink" title="更改预训练任务"></a>更改预训练任务</h5><p>Roberta就是加大数据量，取消NSP任务，更大的batch_size</p>
<p>ERNIE就是更改mask策略，使用短语mask和实体mask</p>
<p>ELECTRA就是预测一个token是否被替换</p>
<p>XLNET将自回归语言模型和双向语言模型结合，具体方式就是排列输入的顺序，使得看到的上文信息有下文的信息</p>
<h4 id="我认为未来的优化方向"><a href="#我认为未来的优化方向" class="headerlink" title="我认为未来的优化方向"></a>我认为未来的优化方向</h4><p>分为三个阶段，第一个阶段就是大规模同样语料的预训练，第二阶段就是在特定领域引入知识表示，最后一阶段才是在该领域的任务上进行fine-tune</p>
<h4 id="ELMO-GPT-BERT对比"><a href="#ELMO-GPT-BERT对比" class="headerlink" title="ELMO GPT BERT对比"></a>ELMO GPT BERT对比</h4><p>ELMO主要是三层，第一层也就是输入层，采用就是word embedding作为输入，然后另外两层是双向lstm，分别是做一个前向语言模型，和后向语言模型，最大化前向和后向的概率和，然后来训练语言模型。最后针对下游任务的时候可以将三层embedding加权叠加，这样的话其实就相当于做了一个动态的word embedding</p>
<p>GPT是单向自回归语言模型，他是使用transformer 的decoder模块(去掉encoder-decoder之间的attention)来作为编码器，这样的话就可以利用之前的信息来预测下一个单词</p>
<p>BERT改进了gpt，通过自编码的思想，实现了双向语言模型的目的，使用上下文来预测当前的单词。</p>
<h3 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h3><p>word2vec是一种将词映射到一个低纬稠密向量空间的方法，也就是词嵌入。它主要包含两种，CBOW和skip-gram，CBOW就是扣掉某词，用上下文来预测它。skip-gram相反。</p>
<p>word2vec有两种优化方式，第一种叫层次softmax，第二种是负采样。</p>
<h4 id="层次softmax"><a href="#层次softmax" class="headerlink" title="层次softmax"></a>层次softmax</h4><p>首先根据训练语料的词典构建霍夫曼树，这样的话高频词就处在较前面的位置。霍夫曼树的根节点是隐藏层的输出向量，然后使用逻辑回归计算概率。每个非叶子节点都是一个向量。就是本来的隐藏层到输出层这个矩阵参数变成了非叶子结点。为什么使用层次softmax呢，因为普通的softmax要对整个词典去计算概率，然后取最大的那个词，这样太耗费资源。而使用霍夫曼树，因为我训练是知道他的target word的，所以我可以快速找到这个词，也就是霍夫曼树的某个节点，然霍夫曼树每条路径其实都是概率的连乘，这样我们就可以最大化那个目标词的概率就好了。</p>
<h5 id="霍夫曼树的构建"><a href="#霍夫曼树的构建" class="headerlink" title="霍夫曼树的构建"></a>霍夫曼树的构建</h5><p>就是每次找两个最小的结点，然后构造根节点是两个结点之和。再把这个根节点放入原数据，在找两个最小的结点。</p>
<h4 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h4><p>负采样的思想就是我每次都更新所有词的向量，这样也太耗时了。所以我能不能每次只更新部分，然后目标词是正样本，这个肯定要更新。而其他的词呢就是负样本，我可以以一个概率去选择更新哪些词，这个概率可以与词频成正比。</p>
<h5 id="负采样比较容易采样到高频词解决办法"><a href="#负采样比较容易采样到高频词解决办法" class="headerlink" title="负采样比较容易采样到高频词解决办法"></a>负采样比较容易采样到高频词解决办法</h5><p>对词频开3/4幂，这样的话会对高频词影响较大，把它的频率拉下来，提高一下低频词被采样到的概率。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfgmusjjicj30bo04ajri.jpg" alt="image-20200604223721015"></p>
<h4 id="word2vec的gensim库训练"><a href="#word2vec的gensim库训练" class="headerlink" title="word2vec的gensim库训练"></a>word2vec的gensim库训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gensim.models.word2vec.Word2Vec(sentences=<span class="literal">None</span>, size=<span class="number">100</span>, alpha=<span class="number">0.025</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<h4 id="其他的文本表示方法"><a href="#其他的文本表示方法" class="headerlink" title="其他的文本表示方法"></a>其他的文本表示方法</h4><p>主要分为两大类，离散表示，分布式表示</p>
<p>分布式表示就是word2vec,glove这种</p>
<ul>
<li>glove词向量就是考虑的共现信息，比如三个词，i,j,k， p(i,k)很小，但是比率比较大，通过比率也可以看出哪两个词更相关，所以这时候模型就可以对i,j,k这三个词进行建模，拟合的目标就是他们之间的比值</li>
</ul>
<p>离散表示就是词袋模型，tf-idf表示，n-gram表示</p>
<ul>
<li><p>词袋模型，没有词的顺序，构建一个词典，出现了某个词，就在那个index置1</p>
</li>
<li><p>Tf-idf，还是构建一个词典纬度的向量，tf就是该词在该文档中的频率，idf就是逆文档频率</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdc6f9dg7ij315o0asabp.jpg" alt="image-20200330192554917"></p>
</li>
<li><p>N-gram 就是每n个连续的词也加入到词典，在去进行下面同样的操作</p>
</li>
</ul>
<h3 id="为什么神经网络初始权重不能为0"><a href="#为什么神经网络初始权重不能为0" class="headerlink" title="为什么神经网络初始权重不能为0"></a>为什么神经网络初始权重不能为0</h3><p>如果为0，会导致后续隐藏层的所有神经元的值都是一样的，然后梯度下降的时候，梯度改变值也是一样的，那么就会造成神经元不能学习不同的特征。</p>
<h3 id="过拟合及优化专项"><a href="#过拟合及优化专项" class="headerlink" title="过拟合及优化专项"></a>过拟合及优化专项</h3><h4 id="什么是过拟合"><a href="#什么是过拟合" class="headerlink" title="什么是过拟合"></a>什么是过拟合</h4><p>当模型在训练集上面性能越来越高，但在测试集上不增反降，这种现象就是过拟合</p>
<h4 id="过拟合的原因"><a href="#过拟合的原因" class="headerlink" title="过拟合的原因"></a>过拟合的原因</h4><ul>
<li>数据量少</li>
<li>模型复杂</li>
</ul>
<h4 id="如何改善"><a href="#如何改善" class="headerlink" title="如何改善"></a>如何改善</h4><p>外部：增加数据量，earlystopping</p>
<p>内部：dropout，L1、L2正则，batchnormalization，集成模型</p>
<h4 id="L1、L2正则"><a href="#L1、L2正则" class="headerlink" title="L1、L2正则"></a>L1、L2正则</h4><p>两种解释：1. 公式推导 2.损失值等高线图角度</p>
<p>按照公式在0处求导，原始的损失函数假设是d，那么加入L2正则后还是d，而加入L1正则会变成d-c和d+c，如果异号，那么0就是极值点，所以容易产生稀疏解</p>
<p>带正则项和带约束条件是等价的</p>
<p>为什么L1，L2相当于缩小了解空间呢？</p>
<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdmcb61s8dj30u00v41ky.jpg" alt="image-20200408142459145" style="zoom:50%;" />

<p>dropout和batch_normalization查看上文</p>
<h3 id="Attention总结"><a href="#Attention总结" class="headerlink" title="Attention总结"></a>Attention总结</h3><p>分为s2s,self-attention,multihead-attention</p>
<p>这个网上资料有分类成soft attention(就是常规attention)和hard attention(不可导)，根据计算区域分为global attention (对所有的k计算attention)和 local attention(窗口内部分k做attention)。我自己还是比较倾向于分成两个大类，第一个就是encoder和decoder之间的这种attention，第二类就是自己和自己的attention</p>
<p>attention一开始在NLP里的应用是seq2seq这种结构。原始的做法一般就是encoder把输入编码成一个固定的context，然后交由解码器去解码。但是这种显然是不合理的，解码的每一个状态应该与输入端的不同状态联系是不一样的，所以呢我在解码端就去用隐状态和输入端的每一个状态做一个相似度计算，这里的相似度计算可以用乘性的attention也可以用加性的attention，比如我们这里用点积，计算完相似度后，我们对这个向量进行softmax，相当于是一个权重，然后对输入端计算一个加权和，那么这个context就和解码端的联系就比较紧密了，解码端不同的状态可以关注到不同的输入端信息。</p>
<p>后来一个就是出来了self-attention，这个就是顾名思义，就是自己和自己做一个attention。比如一个句子，将每个token去和其他的token做一个attention,然后加权求和，这样的话，每个token都相当于与其他的token都形成了联系。这比rnn好的地方就是rnn长距离依赖不行，但是self-attention无视距离关系，所以没有这种缺点。</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdmnkds2cfj30zo0bmq9n.jpg" alt="image-20200408205427460"></p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdmnkuii10j30kw06675i.jpg" alt="image-20200408205457419"></p>
<h3 id="为什么分类模型用交叉熵损失，回归模型用MSE损失"><a href="#为什么分类模型用交叉熵损失，回归模型用MSE损失" class="headerlink" title="为什么分类模型用交叉熵损失，回归模型用MSE损失"></a>为什么分类模型用交叉熵损失，回归模型用MSE损失</h3><p>首先要知道，分类问题一般是用softmax做激活函数，而这里使用交叉熵作为损失函数一般是要比MSE更能收敛到一个好的极值点的。</p>
<p>损失函数角度：交叉熵只关注于特定的类别分量，而MSE考虑所有类别</p>
<p>梯度角度：</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gdrx95mufrj31ds0f4div.jpg" alt="image-20200413101722976"></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gdrx9i1e5wj30ni03kaa7.jpg" alt="image-20200413101745367"></p>
<h3 id="什么是梯度下降"><a href="#什么是梯度下降" class="headerlink" title="什么是梯度下降"></a>什么是梯度下降</h3><p>梯度是一个函数变化最快的方向，而负梯度就是一个函数下降最快的方向，梯度下降法就是每次计算当前函数的负梯度，然后沿着这个方向走，来达到不断减小目标函数的目的</p>
<h3 id="什么是反向传播"><a href="#什么是反向传播" class="headerlink" title="什么是反向传播"></a>什么是反向传播</h3><p>对于一个有多个隐藏层的神经网络，输出层的误差信息是无法直接反映到中间层的权重上的，反向传播其实就是利用链式求导法则将输出层的误差传播到中间层的权重上，让中间层的权重得以更新参数</p>
<h3 id="梯度消失-爆炸原因"><a href="#梯度消失-爆炸原因" class="headerlink" title="梯度消失/爆炸原因"></a>梯度消失/爆炸原因</h3><p>首先梯度消失/爆炸是网络的权重和激活函数一起作用的结果，当梯度反向传播的时候，会有激活函数的导数连乘和权重的连乘，比如说这里使用sigmoid函数的话，因为sigmoid饱和区域也就是梯度接近0的区域比较大，所以连乘后容易造成梯度消失。而如果权重比较大的话，就容易造成梯度爆炸。</p>
<p>梯度爆炸相对来说容易解决，可以使用梯度裁剪</p>
<p>梯度消失比较难搞，可以换激活函数，使用BN层等方式</p>
<h3 id="激活函数总结"><a href="#激活函数总结" class="headerlink" title="激活函数总结"></a>激活函数总结</h3><p>首先理解激活函数的作用，为模型引入非线性表示，如果没有非线性表示的话，无论多么深的网络最后其实都可以写成一个线性组合，就是乘一个权重加上偏置。</p>
<p>那么sigmoid,tanh激活函数缺点是什么呢？就是敏感区域也就是梯度值比较大的区域比较小，网络比较深的时候很容易造成梯度消失。</p>
<p>relu只是可以减缓这种现象，虽然在负数那一侧也会梯度为0，但是比较少。更多的还是在正数一侧。relu还有一个好处就是可以使得网络具有稀疏性，稀疏性为什么好，我的看法是这种东西跟dropout差不多，使得模型更关注典型的特征，摒弃掉那些无效的特征。另外relu可以加速网络的训练，因为他的导数相比sigmoid来说容易计算很多。</p>
<h3 id="神经网络偏置的作用"><a href="#神经网络偏置的作用" class="headerlink" title="神经网络偏置的作用"></a>神经网络偏置的作用</h3><p>不加偏置，必然经过原点。加了偏置其实就是增强了表达能力</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chen shuai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小二郎">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" itemprop="url">机器学习常见问题</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-07T12:47:01+08:00">
                2020-06-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><h3 id="基本名词解释"><a href="#基本名词解释" class="headerlink" title="基本名词解释"></a>基本名词解释</h3><h4 id="1-什么是生成模型，什么是判别模型"><a href="#1-什么是生成模型，什么是判别模型" class="headerlink" title="1. 什么是生成模型，什么是判别模型"></a>1. 什么是生成模型，什么是判别模型</h4><p>简单点来说，生成模型学习联合概率分布，然后求解条件概率分布；判别模型没有这个学习概率的过程，是直接学习决策函数或者条件概率分布</p>
<img src="https://tva1.sinaimg.cn/large/0082zybply1gc7n9z5jnej30vs0qaqgc.jpg" alt="image-20200224180054624" style="zoom:50%;" />

<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcpsw3i3qij31920i2q8l.jpg" alt="image-20200311105517763"></p>
<h4 id="2-什么是回归，什么是分类"><a href="#2-什么是回归，什么是分类" class="headerlink" title="2. 什么是回归，什么是分类"></a>2. 什么是回归，什么是分类</h4><p>分类问题就是预测值y是一个离散有限的变量，通过数据学习一个决策函数，将未知数据预测到对应类别</p>
<p>回归问题就是待预测的y值是一个连续的变量</p>
<h4 id="3-最大似然估计"><a href="#3-最大似然估计" class="headerlink" title="3. 最大似然估计"></a>3. 最大似然估计</h4><p>似然函数：p(data|theta)，给定参数theta(值未知)下data发生的概率</p>
<p>参数固定，只是不知道值，现在通过给定的一组观察结果，写出观测样本的似然函数，最大化似然函数，求这个参数使得出现这组观察结果概率最大</p>
<h4 id="4-最大后验估计"><a href="#4-最大后验估计" class="headerlink" title="4. 最大后验估计"></a>4. 最大后验估计</h4><p>最大后验估计就是假定这个参数应该符合一定的先验分布，并不是一个值，是一个概率分布。通过贝叶斯公式可以写出后验概率的计算，与似然和先验概率有关，所以最大后验估计是不仅要保证似然大，还要保证先验概率也大。相当于加了一个约束项。</p>
<h4 id="5-贝叶斯估计"><a href="#5-贝叶斯估计" class="headerlink" title="5. 贝叶斯估计"></a>5. 贝叶斯估计</h4><p>跟最大后验估计很像，不过最后求的是参数的分布，而不是具体的值</p>
<h4 id="6-什么是最小二乘法"><a href="#6-什么是最小二乘法" class="headerlink" title="6. 什么是最小二乘法"></a>6. 什么是最小二乘法</h4><p>最小二乘法，也叫最小平方，它的目标是最小化误差的平方和。</p>
<p>通过使得误差平方和最小来求解参数，求解参数的思路就是误差值对参数求导，然后令导数等于0，即可求得参数</p>
<h4 id="7-最大似然和最小二乘法估计例题"><a href="#7-最大似然和最小二乘法估计例题" class="headerlink" title="7. 最大似然和最小二乘法估计例题"></a>7. 最大似然和最小二乘法估计例题</h4><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1geaqchlo3ej316s0u0gt2.jpg" alt="image-20200429164422373"></p>
<h3 id="K近邻"><a href="#K近邻" class="headerlink" title="K近邻"></a>K近邻</h3><p>K近邻是个监督学习方法，既可以做回归，也可以做分类</p>
<p>它是通过计算待预测样本与所有训练样本的距离，然后选取k个最近的训练样本，根据某种决策策略来选定最终的预测类别</p>
<p>这里的距离可以是曼哈顿距离，欧式距离</p>
<p>k值的选择可以通过交叉验证来选定，k值较小的话容易受到噪声影响，形成过拟合；k值过大会形成欠拟合</p>
<p>决策方式一般就是投票，或者带权的投票(根据与样本的远近设置权重)，这里的依据其实就是经验风险最小化</p>
<h4 id="为什么用欧式距离-不用曼哈顿距离"><a href="#为什么用欧式距离-不用曼哈顿距离" class="headerlink" title="为什么用欧式距离 不用曼哈顿距离"></a>为什么用欧式距离 不用曼哈顿距离</h4><p>应该没有这个说法，都可以用。曼哈顿距离对异常值的敏感程度较低</p>
<h4 id="kd树"><a href="#kd树" class="headerlink" title="kd树"></a>kd树</h4><p>k近邻是一种思想，具体还要实现。当然最简单的方法就是线性扫描，但是一旦数据多了起来，这就太耗时了。</p>
<p>因此为了减少计算距离的次数，可以采用特殊的结构存储训练数据，如kd树。用来对k维空间的数据进行快速检索</p>
<h5 id="kd树的构造过程"><a href="#kd树的构造过程" class="headerlink" title="kd树的构造过程"></a>kd树的构造过程</h5><img src="https://tva1.sinaimg.cn/large/0082zybply1gc7sjust01j30xy0p20y2.jpg" alt="image-20200224210307419" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/0082zybply1gc7sl3naj5j30tg01ut9e.jpg" alt="image-20200224210431328" style="zoom:50%;" />

<p>节点意味着一个区域</p>
<p>这样构造后，其实所有的实例点都是在超平面上，对应到上图的话就是在线上</p>
<h5 id="kd树的搜索"><a href="#kd树的搜索" class="headerlink" title="kd树的搜索"></a>kd树的搜索</h5><p>一般来说，搜索时间复杂度是O(logN)，但是如果维度太大，会退化为线性搜索</p>
<img src="https://tva1.sinaimg.cn/large/0082zybply1gc7swir97xj30wq0s2qb7.jpg" alt="image-20200224211528669" style="zoom:50%;" />

<p><u><strong><em>搜索还要在看一下，有点迷糊</em></strong></u></p>
<h3 id="kmeans聚类算法"><a href="#kmeans聚类算法" class="headerlink" title="kmeans聚类算法"></a>kmeans聚类算法</h3><h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><h4 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h4><p>贝叶斯分类器就是利用贝叶斯法则，计算这个属于哪个类，使得它的后验概率最大。</p>
<h4 id="贝叶斯法则"><a href="#贝叶斯法则" class="headerlink" title="贝叶斯法则"></a>贝叶斯法则</h4><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7u63yqxpj31i50u0tut.jpg" alt="IMG_5130" style="zoom: 25%;" />

<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7u6ywhkyj30pu01swes.jpg" alt="image-20200224220012429" style="zoom:50%;" />

<p>因为上式太难求解了，所以需要来一个强力的假设，就是条件独立，因此叫朴素贝叶斯</p>
<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7u9d6v41j30lw048wez.jpg" alt="image-20200224220229865" style="zoom:50%;" />

<p>最后就是计算一个后验概率，输出后验概率概率最大的那个类</p>
<p>后验概率最大化其实就是等价于期望风险最小化，这也就是朴素贝叶斯背后的原理</p>
<h4 id="朴素贝叶斯的参数估计"><a href="#朴素贝叶斯的参数估计" class="headerlink" title="朴素贝叶斯的参数估计"></a>朴素贝叶斯的参数估计</h4><p>这个分类器其实就是学习两个东西，一个条件概率，一个先验概率，也就是：</p>
<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7umgrqyaj30mc05gdkk.jpg" alt="image-20200224221505772" style="zoom:50%;" />

<p>这里呢就有两种参数估计方法</p>
<h5 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h5><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7unlauggj30hk03q0t0.jpg" alt="image-20200224221611363" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7uold10vj30vw0du77f.jpg" alt="image-20200224221708574" style="zoom:50%;" />

<h5 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h5><p>由于极大似然估计可能出现概率值为0的情况，所以引入一个参数来变成贝叶斯估计</p>
<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7uu3f3lvj30nu05wt9m.jpg" alt="image-20200224222224667" style="zoom:50%;" />

<p>S(j)代表第j个特征取值的个数</p>
<p>lambda=0就是极大似然估计，=1就是拉普拉斯平滑</p>
<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7v0m6lscj30q0066t9r.jpg" alt="image-20200224222842189" style="zoom:50%;" />

<h3 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h3><h4 id="什么是逻辑斯蒂回归"><a href="#什么是逻辑斯蒂回归" class="headerlink" title="什么是逻辑斯蒂回归"></a>什么是逻辑斯蒂回归</h4><p>逻辑回归是一种假设数据服从伯努利分布，并且通过极大化似然函数，使用梯度下降法来进行求解优化，达到对数据进行二分类的目的的模型</p>
<h4 id="逻辑斯蒂函数形式"><a href="#逻辑斯蒂函数形式" class="headerlink" title="逻辑斯蒂函数形式"></a>逻辑斯蒂函数形式</h4><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc9sx5uv4sj30g802a74e.jpg" alt="image-20200226144716617" style="zoom:50%;" />

<h4 id="逻辑斯蒂回归模型形式"><a href="#逻辑斯蒂回归模型形式" class="headerlink" title="逻辑斯蒂回归模型形式"></a>逻辑斯蒂回归模型形式</h4><p>假设 ℎ𝜃(𝑥)为样本为正的概率，则1−ℎ𝜃(𝑥)为样本为负的概率</p>
<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc9syoiielj30ea04gwej.jpg" alt="image-20200226144846751" style="zoom:50%;" />

<h4 id="逻辑斯蒂优化函数，即损失函数"><a href="#逻辑斯蒂优化函数，即损失函数" class="headerlink" title="逻辑斯蒂优化函数，即损失函数"></a>逻辑斯蒂优化函数，即损失函数</h4><p>逻辑斯蒂回归的优化函数是似然函数，极大化似然函数</p>
<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc9t09q4t4j30oq05it8z.jpg" alt="image-20200226145018408" style="zoom:50%;" />

<p>然后转成对数似然</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc9t3foz4qj30h607igmp.jpg" alt="image-20200226145321097"></p>
<p>它的损失函数也可以是交叉熵，就是差了1/m而已</p>
<h4 id="逻辑回归梯度"><a href="#逻辑回归梯度" class="headerlink" title="逻辑回归梯度"></a>逻辑回归梯度</h4><p>g = (h(x)-y)*x</p>
<h4 id="逻辑回归的并行实现"><a href="#逻辑回归的并行实现" class="headerlink" title="逻辑回归的并行实现"></a>逻辑回归的并行实现</h4><ul>
<li>按行并行<br>这个其实就是将样本分到多个机器分别计算，然后求和取平均</li>
<li>按列并行<br>有的特征纬度达到千万甚至亿级，就可以将特征的不同分量分到不同的机器上面，每个机器只负责一部分特征权重的更新</li>
</ul>
<h4 id="逻辑回归可以做多分类吗"><a href="#逻辑回归可以做多分类吗" class="headerlink" title="逻辑回归可以做多分类吗"></a>逻辑回归可以做多分类吗</h4><p>如果多个类别之间不是互斥的关系，那么可以用多个逻辑回归来做多分类，其实就是多标签分类。如果类别之间是互斥的，那么就得用softmax回归。</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc9tlq9un1j31ck0kqq5w.jpg" alt="image-20200226151056171"></p>
<h4 id="逻辑斯蒂回归可以用来处理非线性分类问题吗"><a href="#逻辑斯蒂回归可以用来处理非线性分类问题吗" class="headerlink" title="逻辑斯蒂回归可以用来处理非线性分类问题吗"></a>逻辑斯蒂回归可以用来处理非线性分类问题吗</h4><ol>
<li>不同的特征进行组合，也可以起到非线性的效果</li>
<li>核函数</li>
</ol>
<p>可以，加入核函数</p>
<p>将特征从低维空间转换到高维空间</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfrzeg0knij30jz03ldgp.jpg" alt="image-20200614181239071"></p>
<h4 id="逻辑斯蒂回归为什么要对特征进行归一化"><a href="#逻辑斯蒂回归为什么要对特征进行归一化" class="headerlink" title="逻辑斯蒂回归为什么要对特征进行归一化"></a>逻辑斯蒂回归为什么要对特征进行归一化</h4><p>因为如果各个特征值的量纲不在同一范围的话，容易造成某个特征值对结果影响太大</p>
<h4 id="逻辑回归加入正则化"><a href="#逻辑回归加入正则化" class="headerlink" title="逻辑回归加入正则化"></a>逻辑回归加入正则化</h4><p>正则化的目的就是对系数进行惩罚，防止模型过大</p>
<p>加入L1正则，就是lasso回归</p>
<p>加入L2正则，就是ridge回归</p>
<p>L1正则会产生一个稀疏的解，L2正则会使得参数很小</p>
<h4 id="逻辑回归为什么用sigmoid函数"><a href="#逻辑回归为什么用sigmoid函数" class="headerlink" title="逻辑回归为什么用sigmoid函数"></a>逻辑回归为什么用sigmoid函数</h4><p>首先logit函数单调递增，容易求导</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc9vh0d07cj319i0aqjud.jpg" alt="image-20200226161535757"></p>
<h4 id="逻辑回归为什么用交叉熵损失，不用MSE；线性回归用MSE"><a href="#逻辑回归为什么用交叉熵损失，不用MSE；线性回归用MSE" class="headerlink" title="逻辑回归为什么用交叉熵损失，不用MSE；线性回归用MSE"></a>逻辑回归为什么用交叉熵损失，不用MSE；线性回归用MSE</h4><p>两个方面的原因：</p>
<ol>
<li>逻辑回归使用了sigmoid激活函数，如果使用MSE作为损失函数，是非凸的；而使用交叉熵，损失函数是凸函数，更容易求解到极值点</li>
<li>从梯度来看，使用MSE梯度要乘上一个sigmoid的导数，容易受到这个导数的影响；而使用交叉熵，梯度只与标签和预测值的误差有关，而且误差越大，梯度越大</li>
</ol>
<p>补充：对于一般的神经网络来说，分类也是使用交叉熵，这个是因为softmax激活后，使用交叉熵最后求解的参数值往往更好</p>
<p>线性回归用MSE可以从正态分布角度考虑</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdmb5ie0jdj313y0fu77f.jpg" alt="image-20200408134458870"></p>
<p>我们现在要做的就是当前条件下使得这个概率连乘最大，也就是似然函数最大。所以就取对数，然后求导，推出</p>
<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdmb7f20d2j30f00b2t9l.jpg" alt="image-20200408134651276" style="zoom:33%;" />

<p>我们希望e越小越好，e满足正态分布，那当然就是u和theta都逼近0好了，那么就推出MSE了</p>
<h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><h4 id="支持向量机优缺点"><a href="#支持向量机优缺点" class="headerlink" title="支持向量机优缺点"></a>支持向量机优缺点</h4><p>优点：适用于特征维数大于样本数量的情况</p>
<h4 id="SVM为什么不用梯度下降"><a href="#SVM为什么不用梯度下降" class="headerlink" title="SVM为什么不用梯度下降"></a>SVM为什么不用梯度下降</h4><p>SVM也可以用梯度下降，不过SVM的目标函数是个凸优化问题可以采用SMO方法求解，速度更快</p>
<h4 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h4><p>当数据集无法线性可分时，可以将它转换到另一个特征空间</p>
<p>这个转换函数就是核函数，本来应该是x和xi都要做一个函数转换在进行内积的，但是这样太耗费空间，所以直接找到一个函数f(x,xi)来达成对两个数分别转换在内积的效果。</p>
<p>通常, 当特征维数 d 超过样本数 m 时 (文本分类问题通常是这种情况), 使用线性核; 当特征维数 d 比较小. 样本数 m 中等时, 使 用 RBF 核; 当特征维数 d 比较小. 样本数 m 特别大时</p>
<h4 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1geovrj19r7j30pq10kqv5.jpg" alt="image-20200511222944254" style="zoom: 50%;" />

<h4 id="SMO优化"><a href="#SMO优化" class="headerlink" title="SMO优化"></a>SMO优化</h4><p>先初始化一对初值，主要思想就是每次选择两个alpha 然后其他alpha值当作常数固定，然后去迭代求解，直到收敛</p>
<h4 id="SVM为什么要用对偶问题来求解"><a href="#SVM为什么要用对偶问题来求解" class="headerlink" title="SVM为什么要用对偶问题来求解"></a>SVM为什么要用对偶问题来求解</h4><p>主要是改变了求解问题的复杂度，原问题与样本纬度有关，而对偶问题与样本数量（支持向量）有关</p>
<h4 id="SVM损失函数"><a href="#SVM损失函数" class="headerlink" title="SVM损失函数"></a>SVM损失函数</h4><p>第一个角度，即统计学习方法推导角度，最大化margin，同时对于一些异常点要有容错能力，引入了松弛变量，那么此时的损失函数为</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gg8dd61okyj30mc0763zc.jpg" alt="image-20200628222451270"></p>
<p>第二个角度，hinge损失，合页损失角度，每个样本带来了1-y*(wx+b)的损失，同时引入L2正则化项</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gg8dgvwvwmj30kq03kq38.jpg" alt="image-20200628222828257"></p>
<h4 id="LR和SVM都是线性分类器"><a href="#LR和SVM都是线性分类器" class="headerlink" title="LR和SVM都是线性分类器"></a>LR和SVM都是线性分类器</h4><p>这是由于他们的决策边界都是y=w*x+b</p>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>决策树是一种基本的分类与回归方法，决策树的学习包含三个步骤：特征选择、决策树的生成、决策树的剪枝</p>
<p>比较典型的决策树模型有ID3,C4.5,CART</p>
<h4 id="ID3决策树"><a href="#ID3决策树" class="headerlink" title="ID3决策树"></a>ID3决策树</h4><p>先介绍熵与条件熵</p>
<p>熵表示一个系统的不确定程度，也就是混乱程度，其公式定义为如下。可以看出，概率越平均，混乱程度越大，熵越高。</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcou5p3u9cj30vw082wgq.jpg" alt="image-20200310145333259"></p>
<p>条件熵就是表示在已知一个条件的情况下，另外一个随机变量的熵，公式定义为：</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcou9fufrhj30vs046mxp.jpg" alt="image-20200310145711831"></p>
<p><strong>信息增益</strong>表示在已知一个特征X的情况下，Y的熵与Y的条件熵差值，信息增益越大就表示这个特征让这个系统更可确定。这个信息增益还有一个说法叫<strong>互信息</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcouedcr8aj30di020wek.jpg" alt="image-20200310150156210"></p>
<p>ID3决策树就是通过信息增益来选择特征分类点的，ID3决策树的缺点就是不能处理连续变量</p>
<h4 id="C4-5决策树"><a href="#C4-5决策树" class="headerlink" title="C4.5决策树"></a>C4.5决策树</h4><p><strong>信息增益比</strong></p>
<p>以信息增益来划分数据集有一个问题，信息增益容易选取那些取值多的特征，比如unique id，取这个特征，直接条件熵为0，显然是不正常的。所以呢要对这个问题矫正，直观的想法肯定是利用这个特征的取值个数进行约束。</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcoukyt9vjj30vc062q4r.jpg" alt="image-20200310150816116"></p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTly1gcovi73exuj31g008e76v.jpg" alt="image-20200310154012573"></p>
<p>C4.5使用信息增益比来选定分裂特征，能够处理连续变量，思路就是对连续值特征进行一个排序，然后选取两个相邻的以均值作为阈值划分为两个部分，作为一个离散特征。</p>
<h4 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h4><p>分为预剪枝和后剪枝，预剪枝就是设定树的深度等，后剪枝就是剪去之后在查看准确率是否提高</p>
<h4 id="CART树"><a href="#CART树" class="headerlink" title="CART树"></a>CART树</h4><p>ID3和C4.5都是分类树</p>
<p>CART树可以同时被用于分类和回归问题</p>
<p>CART树一个比较典型的特征是每个节点只分裂两个子节点，是二叉树</p>
<p>作为分类决策树时，使用gini指数最小化原则，gini指数也是类似熵的概念吧，就是值越大，表示这个集合越混乱。Pk为选定某个特征，敲定特征值后，样本类别的概率</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggtpye1kxqj30d40380sr.jpg" alt="image-20200717093657751"></p>
<p>作为回归决策树时，使用平方误差最小化原则，就是分得的两个子节点内数据均方差最小，同时和也最小</p>
<h4 id="GBDT如何衡量特征重要性"><a href="#GBDT如何衡量特征重要性" class="headerlink" title="GBDT如何衡量特征重要性"></a>GBDT如何衡量特征重要性</h4><p>计算每个特征在树中分裂时候不纯度的减少量，减少的越多，说明特征越重要。</p>
<h4 id="为什么说bagging降低了方差，boosting降低了偏差"><a href="#为什么说bagging降低了方差，boosting降低了偏差" class="headerlink" title="为什么说bagging降低了方差，boosting降低了偏差"></a>为什么说bagging降低了方差，boosting降低了偏差</h4><p>首先模型的误差主要就是来源于两方面：偏差和方差</p>
<p>偏差其实就是模型就是学的不好，而方差就是模型可能过拟合了，所以导致方差大</p>
<p>对于方差，可以这样理解，加入有n个随机变量，如果方差是v,假设这n个变量是独立的，那么其均值的方差就是v/n。类比到bagging，但是bagging不是完全独立，但是各个模型之间相关性也是相对比较弱的，所以能有效降低方差。</p>
<p>对于偏差，boosting每次都会去拟合上一次带来的残差，而且各个基分类器是相关的，所以只能降低偏差</p>
<h4 id="随机森林和GBDT的区别联系"><a href="#随机森林和GBDT的区别联系" class="headerlink" title="随机森林和GBDT的区别联系"></a>随机森林和GBDT的区别联系</h4><p>首先两种都是集成学习方法，集成学习的目的就是通过多个学习器结合来增强模型的鲁棒性和泛化能力</p>
<p>随机森林是基于bagging的思想，就是说每次有放回的抽样一些样本，然后在随机抽取一些特征，然后根据这些样本和特征建立一个决策树。然后重复这个过程，建立多个决策树，形成随机森林，最终的结果可以多数表决。随机森林的优势就是易于并行处理。随机森林可以使用分类树也可以使用回归树。</p>
<p>GBDT就是基于boosting的思想建立的。boosting的思想就是每一次分类器的建立都是为了更加关注上一次分类器分类错误的数据，所以这是一种串行建立模型的方法。GBDT使用的是CART回归树，每一次决策树的拟合目标是损失函数的负梯度。GBDT不是投票，是多个分类器相加，是个加法模型。</p>
<p>为什么GBDT拟合负梯度？</p>
<p>把GBDT模型里的基分类器树函数看成是是一个参数，模型的输出值是关于基分类器的，所以梯度就是损失值对树函数求偏导，然后让基分类器去拟合负梯度，从而达到减小损失值的目的。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6cakcptsj315s0u01kx.jpg" alt="image-20200425213520554"></p>
<h3 id="EM算法推导"><a href="#EM算法推导" class="headerlink" title="EM算法推导"></a>EM算法推导</h3><p>首先介绍一下，如果一个概率模型没有隐藏变量，我们可以直接利用极大似然估计来求解参数，但是如果包含隐变量就不好求了，所以要用EM算法，这就是个迭代算法，最终收敛到一个局部最优</p>
<p>EM算法的原始目标是最大化观测数据Y在参数theta下的似然函数，但是写出来的似然函数包含隐变量，不好求解。所以经过转化，变为最大化Q函数，这个Q函数就是观测变量Y，隐变量Z的似然函数，在当前给定的参数theta下对Z的条件概率分布的期望。然后M步就是求解参数使的Q函数最大。一步步迭代直到收敛。</p>
<h4 id="主要流程"><a href="#主要流程" class="headerlink" title="主要流程"></a>主要流程</h4><p>分两步；第一步，求期望，第二步，期望最大化</p>
<ul>
<li>求期望</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd69w60duqj30j404ct98.jpg" alt="image-20200325165220470"></p>
<ul>
<li><p>最大化期望</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd69wqeqsmj30bw028wek.jpg" alt="image-20200325165253046"></p>
</li>
</ul>
<h4 id="完整推导"><a href="#完整推导" class="headerlink" title="完整推导"></a>完整推导</h4><p>口述：当前时刻的似然函数减去上一个时刻i的似然函数，然后通过jensen不等式拿到似然函数的下届。我们希望似然函数越大越好，那么我们只需要不断增大下界。下届然后我们化简一下其实就是期望Q函数</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd69yc52u3j30w807o76f.jpg" alt="image-20200325165425233"></p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd69z4ngv6j30mw02ugm1.jpg" alt="image-20200325165510993"></p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd6a44bt01j30u00y9tj4.jpg" alt="image-20200325165958487"></p>
<h4 id="收敛性证明"><a href="#收敛性证明" class="headerlink" title="收敛性证明"></a>收敛性证明</h4><p>口述：主要是通过证明似然函数是单调增的，这个证明过程就是用下一个时刻的参数减去上一个时刻的似然函数，然后证明这个差值是大于等于0的</p>
<p>最终是能证明收敛到一个稳定点，但是不能保证收敛到极大值点</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd6ab97t9pj30u014x13f.jpg" alt="image-20200325170650399"></p>
<h3 id="HMM隐马尔可夫模型"><a href="#HMM隐马尔可夫模型" class="headerlink" title="HMM隐马尔可夫模型"></a>HMM隐马尔可夫模型</h3><p>HMM包含三个要素，初始状态概率向量pi，状态转移矩阵A，观测概率矩阵B。</p>
<p>有两个基本假设，第一个齐次马尔可夫链假设就是某时刻的隐状态只与前一个隐状态有关，第二个观测独立性假设就是某时刻的观察状态只与该时刻隐状态有关。</p>
<p>包含三个基本问题，</p>
<ul>
<li><p>概率计算问题，给定一个HMM和一个观测序列，求出现这个序列的概率</p>
</li>
<li><p>学习问题，已知观测序列，求HMM的参数</p>
</li>
<li><p>预测问题，给定一个HMM和一个观测序列，求最有可能的那个隐状态序列</p>
</li>
</ul>
<p>对第一个问题可以采用前向和后向算法，就是一个递推</p>
<p>对于前向算法就是递推的一个过程，算出前一个时刻各种状态转移到当前状态的概率再乘上发射概率</p>
<p>对第三个问题的解法就是维特比算法，该算法是动态规划算法，其本质就是如果这条最大概率路径经过某点，那某点到终点的概率也是最大的，不然就矛盾了。</p>
<p>对于求解模型参数，有监督和非监督方法。如果数据被标注，就是有隐状态，那么可以极大似然估计。不然就只能无监督。无监督就是使用EM算法迭代更新参数，主要的一个过程就是</p>
<ul>
<li>先对各个参数进行初始化</li>
<li>E步，然后是求出Q函数，也就是求期望</li>
<li>M步就是求解这个时刻新的参数值，使得此刻Q函数最大</li>
<li>然后重复，直到收敛</li>
</ul>
<h4 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a>维特比算法</h4><p>基于动态规划思想的算法，在计算第k步的时候，利用了k-1步的结果，所以可以减少计算量。</p>
<h3 id="HMM-和-CRF-的对比"><a href="#HMM-和-CRF-的对比" class="headerlink" title="HMM 和 CRF 的对比"></a>HMM 和 CRF 的对比</h3><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd5dpyqbitj30vi07gwgz.jpg" alt="image-20200324221913840"></p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd5dqefzmzj30we054q3l.jpg" alt="image-20200324221939161"></p>
<p>可以从公示看出来，这里的概率是全局归一化的概率，所以得到的解是全局的最优解</p>
<ol>
<li>HMM是有向图模型，CRF是无向图模型</li>
<li>HMM是生成式模型，是计算一个联合概率，最后来求解条件概率；CRF是判别式模型，是直接对条件概率进行建模</li>
<li>HMM由于有观测独立性假设，所以HMM在做标注问题时不能很好的考虑全局的上下文信息；而对CRF来说，抛弃了这个假设，可以通过特征函数捕捉一个上下文信息，所以CRF相比HMM可以更好的利用上下文信息。</li>
<li>CRF最后计算的概率是基于所有可能的序列归一化进行计算的，所以得到的解是全局最优解。</li>
<li>对于参数学习问题，HMM是学习初始概率、转移概率矩阵、发射概率矩阵，这个可以用极大似然估计，也就是从训练数据中去统计；也可以用EM算法去估计参数。对于CRF来说就是学习各个特征函数的权重，这个也是极大似然函数，但是没法统计，最后是要用梯度下降去求解的</li>
<li>在tensorflow里bisltm+crf，bilstm输出的每个token的logits相当于提供的是一个状态函数得分，然后还有一个要学的转移矩阵的参数，这个就是转移函数</li>
<li>CRF++里面是人为定义特征函数模版，包括转移函数、状态函数，然后模型去学习函数前的权重。</li>
</ol>
<h3 id="CRF模型实现细节"><a href="#CRF模型实现细节" class="headerlink" title="CRF模型实现细节"></a>CRF模型实现细节</h3><p>目标肯定是最大化似然函数，计算包括两步，第一步就是这条序列的得分，第二步就是归一化因子Z，损失函数就是负对数似然。</p>
<p>第一步的得分就是算出每个标签的打分，打分是两个组成，一个是特征状态函数得分，这个是bilstm输出的logits；第二个组成是转移函数得分，这个是待学习的参数。状态函数得分和转移函数得分相加就是这条序列的得分。</p>
<p>第二步的归一化因子Z比较难求，见下图</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf05ggwkraj313c0m4436.jpg" alt="image-20200521162621064"></p>
<p>然后加个log函数，把除变成减</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf05fyfkx8j312m07sdhf.jpg" alt="image-20200521162549278"></p>
<h3 id="LR和SVM的对比"><a href="#LR和SVM的对比" class="headerlink" title="LR和SVM的对比"></a>LR和SVM的对比</h3><ol>
<li>LR和SVM都是线性分类模型，且都是判别式模型</li>
<li>LR和SVM也都可以用于非线性分类，只需要加核函数</li>
<li>LR的损失函数是交叉熵，SVM用的是hinge loss，自带正则化</li>
<li>LR参数求解与所有的数据都有关，SVM只与少数的几个支持向量有关</li>
<li>SVM比较适合特征维度高但是数据量小的数据集，不适合数据量大的数据集</li>
</ol>
<h3 id="LR和SVM关于核函数"><a href="#LR和SVM关于核函数" class="headerlink" title="LR和SVM关于核函数"></a>LR和SVM关于核函数</h3><p>首先LR和SVM都是线性分类器，所以在处理非线性分类问题时就得采取其他手段，比如核函数</p>
<p>本来的做法是将低维向量映射到高维空间，再去做内积。但是这种方式有点开销太大，所以引入核函数，相当于直接计算映射到高维空间后的内积。</p>
<p>LR当然也可以引入核函数，但是LR训练时候是考虑所有样本点的，所以开销太大了，因此有另外一种简便的方法，就是对不同的特征进行组合</p>
<h3 id="机器学习归一化"><a href="#机器学习归一化" class="headerlink" title="机器学习归一化"></a>机器学习归一化</h3><p>不需要归一化：树模型，因为是否归一化不影响分裂节点的选择，树模型只关注值的分布</p>
<p>需要归一化：LR SVM。归一化将损失函数的等高线从椭圆变为圆，加快梯度下降的过程。SVM与距离度量有关，所以也要归一化，避免某些特征影响过大。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E7%BB%8F%E9%AA%8C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chen shuai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小二郎">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E7%BB%8F%E9%AA%8C/" itemprop="url">深度学习调参经验</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-04T22:11:42+08:00">
                2020-06-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>深度学习调参主要就是基本的超参数，如学习率、batchsize这些，还有网络的结构参数，如embedding维度、LSTM维度。</p>
<h3 id="学习率"><a href="#学习率" class="headerlink" title="学习率"></a>学习率</h3><p>从小到大，不同的量级依次提高，[0.1,0.01,0.001…….]。还有模型后期的学习率衰减。如果使用预训练词向量的话，可以对embedding层设置一个较小的学习率。以一个学习率训练完整个模型，可以再已较低的学习率重新进一步训练。</p>
<h3 id="BatchSize"><a href="#BatchSize" class="headerlink" title="BatchSize"></a>BatchSize</h3><p>一般就128，256没啥好说的</p>
<h3 id="checkpoint融合"><a href="#checkpoint融合" class="headerlink" title="checkpoint融合"></a>checkpoint融合</h3><p>和Transformer学的骚操作，可以对最后5轮的checkpoint做一个平均，当作模型最后训练的参数值</p>
<h3 id="loss震荡"><a href="#loss震荡" class="headerlink" title="loss震荡"></a>loss震荡</h3><p>三种可能原因</p>
<ol>
<li>数据本身有问题</li>
<li>lr学习率过大，导致无法找到好的极值点</li>
<li>batchsize过小</li>
</ol>
<h3 id="对于结构的维度"><a href="#对于结构的维度" class="headerlink" title="对于结构的维度"></a>对于结构的维度</h3><p>我习惯从小到大，依次提高。比如lstm维度，从100逐步增加到300或者400</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/04/unsupervised-domain-adaptation-of-contextualized-embedding-for-sequence-labeling/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chen shuai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小二郎">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2020/06/04/unsupervised-domain-adaptation-of-contextualized-embedding-for-sequence-labeling/" itemprop="url">unsupervised domain adaptation of contextualized embedding for sequence labeling</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-04T18:53:57+08:00">
                2020-06-04
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>像BERT、ELMO这些预训练的语言模型都是在受限的几个文本上训练的。如果想把这些预训练语言模型引入到其他领域该如何做呢？比如历史文本是18世纪的，这种以前的用语与现在风格差别还是比较大的。作者基于这种情景，提出了AdaptaBERT这一模型。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>场景设置：source domain和基于该domain下的标注数据；target domain和该domain下待标注的数据。source domain和target domain是不同的。</p>
<p>作者提出两阶段的fine-tune策略：</p>
<ol>
<li>Domain tuning：遮蔽语言模型预训练，训练数据是source domain和target domain的数据</li>
<li>Task tuning：针对任务进行的fine-tune, 训练数据是source domain下的标注数据</li>
</ol>
<p>两阶段的fine-tune后，就可以对target domain的数据进行预测了。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">chen shuai</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chen shuai</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
