<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="机器学习," />










<meta name="description" content="机器学习基本名词解释1. 什么是生成模型，什么是判别模型简单点来说，生成模型学习联合概率分布，然后求解条件概率分布；判别模型没有这个学习概率的过程，是直接学习决策函数或者条件概率分布    2. 什么是回归，什么是分类分类问题就是预测值y是一个离散有限的变量，通过数据学习一个决策函数，将未知数据预测到对应类别 回归问题就是待预测的y值是一个连续的变量 3. 最大似然估计似然函数：p(data|th">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习常见问题">
<meta property="og:url" content="http://yoursite.com/2020/06/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/index.html">
<meta property="og:site_name" content="小二郎">
<meta property="og:description" content="机器学习基本名词解释1. 什么是生成模型，什么是判别模型简单点来说，生成模型学习联合概率分布，然后求解条件概率分布；判别模型没有这个学习概率的过程，是直接学习决策函数或者条件概率分布    2. 什么是回归，什么是分类分类问题就是预测值y是一个离散有限的变量，通过数据学习一个决策函数，将未知数据预测到对应类别 回归问题就是待预测的y值是一个连续的变量 3. 最大似然估计似然函数：p(data|th">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybply1gc7n9z5jnej30vs0qaqgc.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcpsw3i3qij31920i2q8l.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1geaqchlo3ej316s0u0gt2.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybply1gc7sjust01j30xy0p20y2.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybply1gc7sl3naj5j30tg01ut9e.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybply1gc7swir97xj30wq0s2qb7.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc7u63yqxpj31i50u0tut.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc7u6ywhkyj30pu01swes.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc7u9d6v41j30lw048wez.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc7umgrqyaj30mc05gdkk.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc7unlauggj30hk03q0t0.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc7uold10vj30vw0du77f.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc7uu3f3lvj30nu05wt9m.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc7v0m6lscj30q0066t9r.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc9sx5uv4sj30g802a74e.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc9syoiielj30ea04gwej.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc9t09q4t4j30oq05it8z.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc9t3foz4qj30h607igmp.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc9tlq9un1j31ck0kqq5w.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/0082zybpgy1gc9vh0d07cj319i0aqjud.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gdmb5ie0jdj313y0fu77f.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gdmb7f20d2j30f00b2t9l.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1geovrj19r7j30pq10kqv5.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcou5p3u9cj30vw082wgq.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcou9fufrhj30vs046mxp.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcouedcr8aj30di020wek.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcoukyt9vjj30vc062q4r.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTly1gcovi73exuj31g008e76v.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6cakcptsj315s0u01kx.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gd69w60duqj30j404ct98.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gd69wqeqsmj30bw028wek.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gd69yc52u3j30w807o76f.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gd69z4ngv6j30mw02ugm1.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gd6a44bt01j30u00y9tj4.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gd6ab97t9pj30u014x13f.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gd5dpyqbitj30vi07gwgz.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gd5dqefzmzj30we054q3l.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf05ggwkraj313c0m4436.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf05fyfkx8j312m07sdhf.jpg">
<meta property="article:published_time" content="2020-06-07T04:47:01.000Z">
<meta property="article:modified_time" content="2020-06-11T08:21:02.413Z">
<meta property="article:author" content="chen shuai">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/0082zybply1gc7n9z5jnej30vs0qaqgc.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/06/07/机器学习常见问题/"/>





  <title>机器学习常见问题 | 小二郎</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">小二郎</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chen shuai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小二郎">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">机器学习常见问题</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-07T12:47:01+08:00">
                2020-06-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><h3 id="基本名词解释"><a href="#基本名词解释" class="headerlink" title="基本名词解释"></a>基本名词解释</h3><h4 id="1-什么是生成模型，什么是判别模型"><a href="#1-什么是生成模型，什么是判别模型" class="headerlink" title="1. 什么是生成模型，什么是判别模型"></a>1. 什么是生成模型，什么是判别模型</h4><p>简单点来说，生成模型学习联合概率分布，然后求解条件概率分布；判别模型没有这个学习概率的过程，是直接学习决策函数或者条件概率分布</p>
<img src="https://tva1.sinaimg.cn/large/0082zybply1gc7n9z5jnej30vs0qaqgc.jpg" alt="image-20200224180054624" style="zoom:50%;" />

<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcpsw3i3qij31920i2q8l.jpg" alt="image-20200311105517763"></p>
<h4 id="2-什么是回归，什么是分类"><a href="#2-什么是回归，什么是分类" class="headerlink" title="2. 什么是回归，什么是分类"></a>2. 什么是回归，什么是分类</h4><p>分类问题就是预测值y是一个离散有限的变量，通过数据学习一个决策函数，将未知数据预测到对应类别</p>
<p>回归问题就是待预测的y值是一个连续的变量</p>
<h4 id="3-最大似然估计"><a href="#3-最大似然估计" class="headerlink" title="3. 最大似然估计"></a>3. 最大似然估计</h4><p>似然函数：p(data|theta)，给定参数theta(值未知)下data发生的概率</p>
<p>参数固定，只是不知道值，现在通过给定的一组观察结果，写出观测样本的似然函数，最大化似然函数，求这个参数使得出现这组观察结果概率最大</p>
<h4 id="4-最大后验估计"><a href="#4-最大后验估计" class="headerlink" title="4. 最大后验估计"></a>4. 最大后验估计</h4><p>最大后验估计就是假定这个参数应该符合一定的先验分布，并不是一个值，是一个概率分布。通过贝叶斯公式可以写出后验概率的计算，与似然和先验概率有关，所以最大后验估计是不仅要保证似然大，还要保证先验概率也大。相当于加了一个约束项。</p>
<h4 id="5-贝叶斯估计"><a href="#5-贝叶斯估计" class="headerlink" title="5. 贝叶斯估计"></a>5. 贝叶斯估计</h4><p>跟最大后验估计很像，不过最后求的是参数的分布，而不是具体的值</p>
<h4 id="6-什么是最小二乘法"><a href="#6-什么是最小二乘法" class="headerlink" title="6. 什么是最小二乘法"></a>6. 什么是最小二乘法</h4><p>最小二乘法，也叫最小平方，它的目标是最小化误差的平方和。</p>
<p>通过使得误差平方和最小来求解参数，求解参数的思路就是误差值对参数求导，然后令导数等于0，即可求得参数</p>
<h4 id="7-最大似然和最小二乘法估计例题"><a href="#7-最大似然和最小二乘法估计例题" class="headerlink" title="7. 最大似然和最小二乘法估计例题"></a>7. 最大似然和最小二乘法估计例题</h4><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1geaqchlo3ej316s0u0gt2.jpg" alt="image-20200429164422373"></p>
<h3 id="K近邻"><a href="#K近邻" class="headerlink" title="K近邻"></a>K近邻</h3><p>K近邻是个监督学习方法，既可以做回归，也可以做分类</p>
<p>它是通过计算待预测样本与所有训练样本的距离，然后选取k个最近的训练样本，根据某种决策策略来选定最终的预测类别</p>
<p>这里的距离可以是曼哈顿距离，欧式距离</p>
<p>k值的选择可以通过交叉验证来选定，k值较小的话容易受到噪声影响，形成过拟合；k值过大会形成欠拟合</p>
<p>决策方式一般就是投票，或者带权的投票(根据与样本的远近设置权重)，这里的依据其实就是经验风险最小化</p>
<h4 id="为什么用欧式距离-不用曼哈顿距离"><a href="#为什么用欧式距离-不用曼哈顿距离" class="headerlink" title="为什么用欧式距离 不用曼哈顿距离"></a>为什么用欧式距离 不用曼哈顿距离</h4><p>应该没有这个说法，都可以用。曼哈顿距离对异常值的敏感程度较低</p>
<h4 id="kd树"><a href="#kd树" class="headerlink" title="kd树"></a>kd树</h4><p>k近邻是一种思想，具体还要实现。当然最简单的方法就是线性扫描，但是一旦数据多了起来，这就太耗时了。</p>
<p>因此为了减少计算距离的次数，可以采用特殊的结构存储训练数据，如kd树。用来对k维空间的数据进行快速检索</p>
<h5 id="kd树的构造过程"><a href="#kd树的构造过程" class="headerlink" title="kd树的构造过程"></a>kd树的构造过程</h5><img src="https://tva1.sinaimg.cn/large/0082zybply1gc7sjust01j30xy0p20y2.jpg" alt="image-20200224210307419" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/0082zybply1gc7sl3naj5j30tg01ut9e.jpg" alt="image-20200224210431328" style="zoom:50%;" />

<p>节点意味着一个区域</p>
<p>这样构造后，其实所有的实例点都是在超平面上，对应到上图的话就是在线上</p>
<h5 id="kd树的搜索"><a href="#kd树的搜索" class="headerlink" title="kd树的搜索"></a>kd树的搜索</h5><p>一般来说，搜索时间复杂度是O(logN)，但是如果维度太大，会退化为线性搜索</p>
<img src="https://tva1.sinaimg.cn/large/0082zybply1gc7swir97xj30wq0s2qb7.jpg" alt="image-20200224211528669" style="zoom:50%;" />

<p><u><strong><em>搜索还要在看一下，有点迷糊</em></strong></u></p>
<h3 id="kmeans聚类算法"><a href="#kmeans聚类算法" class="headerlink" title="kmeans聚类算法"></a>kmeans聚类算法</h3><h3 id="朴素贝叶斯"><a href="#朴素贝叶斯" class="headerlink" title="朴素贝叶斯"></a>朴素贝叶斯</h3><h4 id="贝叶斯分类器"><a href="#贝叶斯分类器" class="headerlink" title="贝叶斯分类器"></a>贝叶斯分类器</h4><p>贝叶斯分类器就是利用贝叶斯法则，计算这个属于哪个类，使得它的后验概率最大。</p>
<h4 id="贝叶斯法则"><a href="#贝叶斯法则" class="headerlink" title="贝叶斯法则"></a>贝叶斯法则</h4><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7u63yqxpj31i50u0tut.jpg" alt="IMG_5130" style="zoom: 25%;" />

<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7u6ywhkyj30pu01swes.jpg" alt="image-20200224220012429" style="zoom:50%;" />

<p>因为上式太难求解了，所以需要来一个强力的假设，就是条件独立，因此叫朴素贝叶斯</p>
<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7u9d6v41j30lw048wez.jpg" alt="image-20200224220229865" style="zoom:50%;" />

<p>最后就是计算一个后验概率，输出后验概率概率最大的那个类</p>
<p>后验概率最大化其实就是等价于期望风险最小化，这也就是朴素贝叶斯背后的原理</p>
<h4 id="朴素贝叶斯的参数估计"><a href="#朴素贝叶斯的参数估计" class="headerlink" title="朴素贝叶斯的参数估计"></a>朴素贝叶斯的参数估计</h4><p>这个分类器其实就是学习两个东西，一个条件概率，一个先验概率，也就是：</p>
<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7umgrqyaj30mc05gdkk.jpg" alt="image-20200224221505772" style="zoom:50%;" />

<p>这里呢就有两种参数估计方法</p>
<h5 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h5><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7unlauggj30hk03q0t0.jpg" alt="image-20200224221611363" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7uold10vj30vw0du77f.jpg" alt="image-20200224221708574" style="zoom:50%;" />

<h5 id="贝叶斯估计"><a href="#贝叶斯估计" class="headerlink" title="贝叶斯估计"></a>贝叶斯估计</h5><p>由于极大似然估计可能出现概率值为0的情况，所以引入一个参数来变成贝叶斯估计</p>
<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7uu3f3lvj30nu05wt9m.jpg" alt="image-20200224222224667" style="zoom:50%;" />

<p>S(j)代表第j个特征取值的个数</p>
<p>lambda=0就是极大似然估计，=1就是拉普拉斯平滑</p>
<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc7v0m6lscj30q0066t9r.jpg" alt="image-20200224222842189" style="zoom:50%;" />

<h3 id="逻辑斯蒂回归"><a href="#逻辑斯蒂回归" class="headerlink" title="逻辑斯蒂回归"></a>逻辑斯蒂回归</h3><h4 id="什么是逻辑斯蒂回归"><a href="#什么是逻辑斯蒂回归" class="headerlink" title="什么是逻辑斯蒂回归"></a>什么是逻辑斯蒂回归</h4><p>逻辑回归是一种假设数据服从伯努利分布，并且通过极大化似然函数，使用梯度下降法来进行求解优化，达到对数据进行二分类的目的的模型</p>
<h4 id="逻辑斯蒂函数形式"><a href="#逻辑斯蒂函数形式" class="headerlink" title="逻辑斯蒂函数形式"></a>逻辑斯蒂函数形式</h4><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc9sx5uv4sj30g802a74e.jpg" alt="image-20200226144716617" style="zoom:50%;" />

<h4 id="逻辑斯蒂回归模型形式"><a href="#逻辑斯蒂回归模型形式" class="headerlink" title="逻辑斯蒂回归模型形式"></a>逻辑斯蒂回归模型形式</h4><p>假设 ℎ𝜃(𝑥)为样本为正的概率，则1−ℎ𝜃(𝑥)为样本为负的概率</p>
<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc9syoiielj30ea04gwej.jpg" alt="image-20200226144846751" style="zoom:50%;" />

<h4 id="逻辑斯蒂优化函数，即损失函数"><a href="#逻辑斯蒂优化函数，即损失函数" class="headerlink" title="逻辑斯蒂优化函数，即损失函数"></a>逻辑斯蒂优化函数，即损失函数</h4><p>逻辑斯蒂回归的优化函数是似然函数，极大化似然函数</p>
<img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc9t09q4t4j30oq05it8z.jpg" alt="image-20200226145018408" style="zoom:50%;" />

<p>然后转成对数似然</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc9t3foz4qj30h607igmp.jpg" alt="image-20200226145321097"></p>
<p>它的损失函数也可以是交叉熵，就是差了1/m而已</p>
<h4 id="逻辑回归梯度"><a href="#逻辑回归梯度" class="headerlink" title="逻辑回归梯度"></a>逻辑回归梯度</h4><p>g = (h(x)-y)*x</p>
<h4 id="逻辑回归可以做多分类吗"><a href="#逻辑回归可以做多分类吗" class="headerlink" title="逻辑回归可以做多分类吗"></a>逻辑回归可以做多分类吗</h4><p>简单的思路可以构建多个二分类逻辑斯蒂回归模型</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc9tlq9un1j31ck0kqq5w.jpg" alt="image-20200226151056171"></p>
<h4 id="逻辑斯蒂回归可以用来处理非线性分类问题吗"><a href="#逻辑斯蒂回归可以用来处理非线性分类问题吗" class="headerlink" title="逻辑斯蒂回归可以用来处理非线性分类问题吗"></a>逻辑斯蒂回归可以用来处理非线性分类问题吗</h4><p>可以，加入核函数</p>
<p>将特征从低维空间转换到高维空间</p>
<p><strong>*<u>此处补充如何加入核函数</u>*</strong></p>
<h4 id="逻辑斯蒂回归为什么要对特征进行归一化"><a href="#逻辑斯蒂回归为什么要对特征进行归一化" class="headerlink" title="逻辑斯蒂回归为什么要对特征进行归一化"></a>逻辑斯蒂回归为什么要对特征进行归一化</h4><p>因为如果各个特征值的量纲不在同一范围的话，容易造成某个特征值对结果影响太大</p>
<h4 id="逻辑回归加入正则化"><a href="#逻辑回归加入正则化" class="headerlink" title="逻辑回归加入正则化"></a>逻辑回归加入正则化</h4><p>正则化的目的就是对系数进行惩罚，防止模型过大</p>
<p>加入L1正则，就是lasso回归</p>
<p>加入L2正则，就是ridge回归</p>
<p>L1正则会产生一个稀疏的解，L2正则会使得参数很小</p>
<h4 id="逻辑回归为什么用sigmoid函数"><a href="#逻辑回归为什么用sigmoid函数" class="headerlink" title="逻辑回归为什么用sigmoid函数"></a>逻辑回归为什么用sigmoid函数</h4><p>首先logit函数单调递增，容易求导</p>
<p><img src="https://tva1.sinaimg.cn/large/0082zybpgy1gc9vh0d07cj319i0aqjud.jpg" alt="image-20200226161535757"></p>
<h4 id="逻辑回归为什么用交叉熵损失，不用MSE；线性回归用MSE"><a href="#逻辑回归为什么用交叉熵损失，不用MSE；线性回归用MSE" class="headerlink" title="逻辑回归为什么用交叉熵损失，不用MSE；线性回归用MSE"></a>逻辑回归为什么用交叉熵损失，不用MSE；线性回归用MSE</h4><p>因为逻辑回归由于引入了sigmoid激活函数，如果使用MSE，损失函数是非凸的，不容易求解</p>
<p>而使用交叉熵函数，其损失函数是凸函数</p>
<p>线性回归用MSE可以从正态分布角度考虑</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdmb5ie0jdj313y0fu77f.jpg" alt="image-20200408134458870"></p>
<p>我们现在要做的就是当前条件下使得这个概率连乘最大，也就是似然函数最大。所以就取对数，然后求导，推出</p>
<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdmb7f20d2j30f00b2t9l.jpg" alt="image-20200408134651276" style="zoom:33%;" />

<p>我们希望e越小越好，e满足正态分布，那当然就是u和theta都逼近0好了，那么就推出MSE了</p>
<h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><h4 id="支持向量机优缺点"><a href="#支持向量机优缺点" class="headerlink" title="支持向量机优缺点"></a>支持向量机优缺点</h4><p>优点：适用于特征维数大于样本数量的情况</p>
<h4 id="SVM为什么不用梯度下降"><a href="#SVM为什么不用梯度下降" class="headerlink" title="SVM为什么不用梯度下降"></a>SVM为什么不用梯度下降</h4><p>SVM也可以用梯度下降，不过SVM的目标函数是个凸优化问题可以采用SMO方法求解，速度更快</p>
<h4 id="核函数"><a href="#核函数" class="headerlink" title="核函数"></a>核函数</h4><p>当数据集无法线性可分时，可以将它转换到另一个特征空间</p>
<p>这个转换函数就是核函数，本来应该是x和xi都要做一个函数转换在进行内积的，但是这样太耗费空间，所以直接找到一个函数f(x,xi)来达成对两个数分别转换在内积的效果。</p>
<p>通常, 当特征维数 d 超过样本数 m 时 (文本分类问题通常是这种情况), 使用线性核; 当特征维数 d 比较小. 样本数 m 中等时, 使 用 RBF 核; 当特征维数 d 比较小. 样本数 m 特别大时</p>
<h4 id="推导过程"><a href="#推导过程" class="headerlink" title="推导过程"></a>推导过程</h4><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1geovrj19r7j30pq10kqv5.jpg" alt="image-20200511222944254" style="zoom: 50%;" />

<h4 id="SMO优化"><a href="#SMO优化" class="headerlink" title="SMO优化"></a>SMO优化</h4><p>先初始化一对初值，主要思想就是每次选择两个alpha 然后其他alpha值当作常数固定，然后去迭代求解，直到收敛</p>
<h4 id="SVM为什么要用对偶问题来求解"><a href="#SVM为什么要用对偶问题来求解" class="headerlink" title="SVM为什么要用对偶问题来求解"></a>SVM为什么要用对偶问题来求解</h4><p>主要是改变了求解问题的复杂度，原问题与样本纬度有关，而对偶问题与样本数量有关</p>
<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><p>决策树是一种基本的分类与回归方法，决策树的学习包含三个步骤：特征选择、决策树的生成、决策树的剪枝</p>
<p>比较典型的决策树模型有ID3,C4.5,CART</p>
<h4 id="ID3决策树"><a href="#ID3决策树" class="headerlink" title="ID3决策树"></a>ID3决策树</h4><p>先介绍熵与条件熵</p>
<p>熵表示一个系统的不确定程度，也就是混乱程度，其公式定义为如下。可以看出，概率越平均，混乱程度越大，熵越高。</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcou5p3u9cj30vw082wgq.jpg" alt="image-20200310145333259"></p>
<p>条件熵就是表示在已知一个条件的情况下，另外一个随机变量的熵，公式定义为：</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcou9fufrhj30vs046mxp.jpg" alt="image-20200310145711831"></p>
<p><strong>信息增益</strong>表示在已知一个特征X的情况下，Y的熵与Y的条件熵差值，信息增益越大就表示这个特征让这个系统更可确定。这个信息增益还有一个说法叫<strong>互信息</strong></p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcouedcr8aj30di020wek.jpg" alt="image-20200310150156210"></p>
<p>ID3决策树就是通过信息增益来选择特征分类点的，ID3决策树的缺点就是不能处理连续变量</p>
<h4 id="C4-5决策树"><a href="#C4-5决策树" class="headerlink" title="C4.5决策树"></a>C4.5决策树</h4><p><strong>信息增益比</strong></p>
<p>以信息增益来划分数据集有一个问题，信息增益容易选取那些取值多的特征，比如unique id，取这个特征，直接条件熵为0，显然是不正常的。所以呢要对这个问题矫正，直观的想法肯定是利用这个特征的取值个数进行约束。</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcoukyt9vjj30vc062q4r.jpg" alt="image-20200310150816116"></p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTly1gcovi73exuj31g008e76v.jpg" alt="image-20200310154012573"></p>
<p>C4.5使用信息增益比来选定分裂特征，能够处理连续变量，思路就是对连续值特征进行一个排序，然后选取两个相邻的以均值作为阈值划分为两个部分，作为一个离散特征。</p>
<h4 id="决策树的剪枝"><a href="#决策树的剪枝" class="headerlink" title="决策树的剪枝"></a>决策树的剪枝</h4><p>分为预剪枝和后剪枝，预剪枝就是设定树的深度等，后剪枝就是剪去之后在查看准确率是否提高</p>
<h4 id="CART树"><a href="#CART树" class="headerlink" title="CART树"></a>CART树</h4><p>ID3和C4.5都是分类树</p>
<p>CART树可以同时被用于分类和回归问题</p>
<p>CART树一个比较典型的特征是每个节点只分裂两个子节点，是二叉树</p>
<p>作为分类决策树时，使用gini指数最小化原则，gini指数也是类似熵的概念吧，就是值越大，表示这个集合越混乱。</p>
<p>作为回归决策树时，使用平方误差最小化原则，就是分得的两个子节点内数据均方差最小，同时和也最小</p>
<h4 id="为什么说bagging降低了方差，boosting降低了偏差"><a href="#为什么说bagging降低了方差，boosting降低了偏差" class="headerlink" title="为什么说bagging降低了方差，boosting降低了偏差"></a>为什么说bagging降低了方差，boosting降低了偏差</h4><p>首先模型的误差主要就是来源于两方面：偏差和方差</p>
<p>偏差其实就是模型就是学的不好，而方差就是模型可能过拟合了，所以导致方差大</p>
<p>对于方差，可以这样理解，加入有n个随机变量，如果方差是v,假设这n个变量是独立的，那么其均值的方差就是v/n。类比到bagging，但是bagging不是完全独立，但是各个模型之间相关性也是相对比较弱的，所以能有效降低方差。</p>
<p>对于偏差，boosting每次都会去拟合上一次带来的残差，而且各个基分类器是相关的，所以只能降低偏差</p>
<h4 id="随机森林和GBDT的区别联系"><a href="#随机森林和GBDT的区别联系" class="headerlink" title="随机森林和GBDT的区别联系"></a>随机森林和GBDT的区别联系</h4><p>首先两种都是集成学习方法，集成学习的目的就是通过多个学习器结合来增强模型的鲁棒性和泛化能力</p>
<p>随机森林是基于bagging的思想，就是说每次有放回的抽样一些样本，然后在随机抽取一些特征，然后根据这些样本和特征建立一个决策树。然后重复这个过程，建立多个决策树，形成随机森林，最终的结果可以多数表决。随机森林的优势就是易于并行处理。随机森林可以使用分类树也可以使用回归树。</p>
<p>GBDT就是基于boosting的思想建立的。boosting的思想就是每一次分类器的建立都是为了更加关注上一次分类器分类错误的数据，所以这是一种串行建立模型的方法。GBDT使用的是CART回归树，每一次决策树的拟合目标是损失函数的负梯度。GBDT不是投票，是多个分类器相加，是个加法模型。</p>
<p>为什么GBDT拟合负梯度？</p>
<p>把GBDT模型里的基分类器树函数看成是是一个参数，模型的输出值是关于基分类器的，所以梯度就是损失值对树函数求偏导，然后让基分类器去拟合负梯度，从而达到减小损失值的目的。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ge6cakcptsj315s0u01kx.jpg" alt="image-20200425213520554"></p>
<h3 id="EM算法推导"><a href="#EM算法推导" class="headerlink" title="EM算法推导"></a>EM算法推导</h3><p>首先介绍一下，如果一个概率模型没有隐藏变量，我们可以直接利用极大似然估计来求解参数，但是如果包含隐变量就不好求了，所以要用EM算法，这就是个迭代算法，最终收敛到一个局部最优</p>
<p>EM算法的原始目标是最大化观测数据Y在参数theta下的似然函数，但是写出来的似然函数包含隐变量，不好求解。所以经过转化，变为最大化Q函数，这个Q函数就是观测变量Y，隐变量Z的似然函数，在当前给定的参数theta下对Z的条件概率分布的期望。然后M步就是求解参数使的Q函数最大。一步步迭代直到收敛。</p>
<h4 id="主要流程"><a href="#主要流程" class="headerlink" title="主要流程"></a>主要流程</h4><p>分两步；第一步，求期望，第二步，期望最大化</p>
<ul>
<li>求期望</li>
</ul>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd69w60duqj30j404ct98.jpg" alt="image-20200325165220470"></p>
<ul>
<li><p>最大化期望</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd69wqeqsmj30bw028wek.jpg" alt="image-20200325165253046"></p>
</li>
</ul>
<h4 id="完整推导"><a href="#完整推导" class="headerlink" title="完整推导"></a>完整推导</h4><p>口述：当前时刻的似然函数减去上一个时刻i的似然函数，然后通过jensen不等式拿到似然函数的下届。我们希望似然函数越大越好，那么我们只需要不断增大下界。下届然后我们化简一下其实就是期望Q函数</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd69yc52u3j30w807o76f.jpg" alt="image-20200325165425233"></p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd69z4ngv6j30mw02ugm1.jpg" alt="image-20200325165510993"></p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd6a44bt01j30u00y9tj4.jpg" alt="image-20200325165958487"></p>
<h4 id="收敛性证明"><a href="#收敛性证明" class="headerlink" title="收敛性证明"></a>收敛性证明</h4><p>口述：主要是通过证明似然函数是单调增的，这个证明过程就是用下一个时刻的参数减去上一个时刻的似然函数，然后证明这个差值是大于等于0的</p>
<p>最终是能证明收敛到一个稳定点，但是不能保证收敛到极大值点</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd6ab97t9pj30u014x13f.jpg" alt="image-20200325170650399"></p>
<h3 id="HMM隐马尔可夫模型"><a href="#HMM隐马尔可夫模型" class="headerlink" title="HMM隐马尔可夫模型"></a>HMM隐马尔可夫模型</h3><p>HMM包含三个要素，初始状态概率向量pi，状态转移矩阵A，观测概率矩阵B。</p>
<p>有两个基本假设，第一个齐次马尔可夫链假设就是某时刻的隐状态只与前一个隐状态有关，第二个观测独立性假设就是某时刻的观察状态只与该时刻隐状态有关。</p>
<p>包含三个基本问题，</p>
<ul>
<li><p>概率计算问题，给定一个HMM和一个观测序列，求出现这个序列的概率</p>
</li>
<li><p>学习问题，已知观测序列，求HMM的参数</p>
</li>
<li><p>预测问题，给定一个HMM和一个观测序列，求最有可能的那个隐状态序列</p>
</li>
</ul>
<p>对第一个问题可以采用前向和后向算法，就是一个递推</p>
<p>对于前向算法就是递推的一个过程，算出前一个时刻各种状态转移到当前状态的概率再乘上发射概率</p>
<p>对第三个问题的解法就是维特比算法，该算法是动态规划算法，其本质就是如果这条最大概率路径经过某点，那某点到终点的概率也是最大的，不然就矛盾了。</p>
<p>对于求解模型参数，有监督和非监督方法。如果数据被标注，就是有隐状态，那么可以极大似然估计。不然就只能无监督。无监督就是使用EM算法迭代更新参数，主要的一个过程就是</p>
<ul>
<li>先对各个参数进行初始化</li>
<li>E步，然后是求出Q函数，也就是求期望</li>
<li>M步就是求解这个时刻新的参数值，使得此刻Q函数最大</li>
<li>然后重复，直到收敛</li>
</ul>
<h4 id="维特比算法"><a href="#维特比算法" class="headerlink" title="维特比算法"></a>维特比算法</h4><p>基于动态规划思想的算法，在计算第k步的时候，利用了k-1步的结果，所以可以减少计算量。</p>
<h3 id="HMM-和-CRF-的对比"><a href="#HMM-和-CRF-的对比" class="headerlink" title="HMM 和 CRF 的对比"></a>HMM 和 CRF 的对比</h3><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd5dpyqbitj30vi07gwgz.jpg" alt="image-20200324221913840"></p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd5dqefzmzj30we054q3l.jpg" alt="image-20200324221939161"></p>
<p>可以从公示看出来，这里的概率是全局归一化的概率，所以得到的解是全局的最优解</p>
<ol>
<li>HMM是有向图模型，CRF是无向图模型</li>
<li>HMM是生成式模型，是计算一个联合概率，最后来求解条件概率；CRF是判别式模型，是直接对条件概率进行建模</li>
<li>HMM由于有观测独立性假设，所以HMM在做标注问题时不能很好的考虑全局的上下文信息；而对CRF来说，抛弃了这个假设，可以通过特征函数捕捉一个上下文信息，所以CRF相比HMM可以更好的利用上下文信息。</li>
<li>CRF最后计算的概率是基于所有可能的序列归一化进行计算的，所以得到的解是全局最优解。</li>
<li>对于参数学习问题，HMM是学习初始概率、转移概率矩阵、发射概率矩阵，这个可以用极大似然估计，也就是从训练数据中去统计；也可以用EM算法去估计参数。对于CRF来说就是学习各个特征函数的权重，这个也是极大似然函数，但是没法统计，最后是要用梯度下降去求解的</li>
<li>在tensorflow里bisltm+crf，bilstm输出的每个token的logits相当于提供的是一个状态函数得分，然后还有一个要学的转移矩阵的参数，这个就是转移函数</li>
<li>CRF++里面是人为定义特征函数模版，包括转移函数、状态函数，然后模型去学习函数前的权重。</li>
</ol>
<h3 id="CRF模型实现细节"><a href="#CRF模型实现细节" class="headerlink" title="CRF模型实现细节"></a>CRF模型实现细节</h3><p>目标肯定是最大化似然函数，计算包括两步，第一步就是这条序列的得分，第二步就是归一化因子Z，损失函数就是负对数似然。</p>
<p>第一步的得分就是算出每个标签的打分，打分是两个组成，一个是特征状态函数得分，这个是bilstm输出的logits；第二个组成是转移函数得分，这个是待学习的参数。状态函数得分和转移函数得分相加就是这条序列的得分。</p>
<p>第二步的归一化因子Z比较难求，见下图</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf05ggwkraj313c0m4436.jpg" alt="image-20200521162621064"></p>
<p>然后加个log函数，把除变成减</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gf05fyfkx8j312m07sdhf.jpg" alt="image-20200521162549278"></p>
<h3 id="LR和SVM的对比"><a href="#LR和SVM的对比" class="headerlink" title="LR和SVM的对比"></a>LR和SVM的对比</h3><ol>
<li>LR和SVM都是线性分类模型，且都是判别式模型</li>
<li>LR和SVM也都可以用于非线性分类，只需要加核函数</li>
<li>LR的损失函数是交叉熵，SVM用的是hinge loss，自带正则化</li>
<li>LR参数求解与所有的数据都有关，SVM只与少数的几个支持向量有关</li>
<li>SVM比较适合特征维度高但是数据量小的数据集，不适合数据量大的数据集</li>
</ol>
<h3 id="机器学习归一化"><a href="#机器学习归一化" class="headerlink" title="机器学习归一化"></a>机器学习归一化</h3><p>不需要归一化：树模型，因为是否归一化不影响分裂节点的选择，树模型只关注值的分布</p>
<p>需要归一化：LR SVM。归一化将损失函数的等高线从椭圆变为圆，加快梯度下降的过程。SVM与距离度量有关，所以也要归一化，避免某些特征影响过大。</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/06/04/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%B0%83%E5%8F%82%E7%BB%8F%E9%AA%8C/" rel="next" title="深度学习调参经验">
                <i class="fa fa-chevron-left"></i> 深度学习调参经验
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2020/06/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" rel="prev" title="深度学习常见问题">
                深度学习常见问题 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">chen shuai</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">7</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#机器学习"><span class="nav-number">1.</span> <span class="nav-text">机器学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#基本名词解释"><span class="nav-number">1.1.</span> <span class="nav-text">基本名词解释</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-什么是生成模型，什么是判别模型"><span class="nav-number">1.1.1.</span> <span class="nav-text">1. 什么是生成模型，什么是判别模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-什么是回归，什么是分类"><span class="nav-number">1.1.2.</span> <span class="nav-text">2. 什么是回归，什么是分类</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-最大似然估计"><span class="nav-number">1.1.3.</span> <span class="nav-text">3. 最大似然估计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-最大后验估计"><span class="nav-number">1.1.4.</span> <span class="nav-text">4. 最大后验估计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-贝叶斯估计"><span class="nav-number">1.1.5.</span> <span class="nav-text">5. 贝叶斯估计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-什么是最小二乘法"><span class="nav-number">1.1.6.</span> <span class="nav-text">6. 什么是最小二乘法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-最大似然和最小二乘法估计例题"><span class="nav-number">1.1.7.</span> <span class="nav-text">7. 最大似然和最小二乘法估计例题</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#K近邻"><span class="nav-number">1.2.</span> <span class="nav-text">K近邻</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么用欧式距离-不用曼哈顿距离"><span class="nav-number">1.2.1.</span> <span class="nav-text">为什么用欧式距离 不用曼哈顿距离</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#kd树"><span class="nav-number">1.2.2.</span> <span class="nav-text">kd树</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#kd树的构造过程"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">kd树的构造过程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#kd树的搜索"><span class="nav-number">1.2.2.2.</span> <span class="nav-text">kd树的搜索</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kmeans聚类算法"><span class="nav-number">1.3.</span> <span class="nav-text">kmeans聚类算法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#朴素贝叶斯"><span class="nav-number">1.4.</span> <span class="nav-text">朴素贝叶斯</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#贝叶斯分类器"><span class="nav-number">1.4.1.</span> <span class="nav-text">贝叶斯分类器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#贝叶斯法则"><span class="nav-number">1.4.2.</span> <span class="nav-text">贝叶斯法则</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#朴素贝叶斯的参数估计"><span class="nav-number">1.4.3.</span> <span class="nav-text">朴素贝叶斯的参数估计</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#极大似然估计"><span class="nav-number">1.4.3.1.</span> <span class="nav-text">极大似然估计</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#贝叶斯估计"><span class="nav-number">1.4.3.2.</span> <span class="nav-text">贝叶斯估计</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#逻辑斯蒂回归"><span class="nav-number">1.5.</span> <span class="nav-text">逻辑斯蒂回归</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是逻辑斯蒂回归"><span class="nav-number">1.5.1.</span> <span class="nav-text">什么是逻辑斯蒂回归</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑斯蒂函数形式"><span class="nav-number">1.5.2.</span> <span class="nav-text">逻辑斯蒂函数形式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑斯蒂回归模型形式"><span class="nav-number">1.5.3.</span> <span class="nav-text">逻辑斯蒂回归模型形式</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑斯蒂优化函数，即损失函数"><span class="nav-number">1.5.4.</span> <span class="nav-text">逻辑斯蒂优化函数，即损失函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归梯度"><span class="nav-number">1.5.5.</span> <span class="nav-text">逻辑回归梯度</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归可以做多分类吗"><span class="nav-number">1.5.6.</span> <span class="nav-text">逻辑回归可以做多分类吗</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑斯蒂回归可以用来处理非线性分类问题吗"><span class="nav-number">1.5.7.</span> <span class="nav-text">逻辑斯蒂回归可以用来处理非线性分类问题吗</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑斯蒂回归为什么要对特征进行归一化"><span class="nav-number">1.5.8.</span> <span class="nav-text">逻辑斯蒂回归为什么要对特征进行归一化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归加入正则化"><span class="nav-number">1.5.9.</span> <span class="nav-text">逻辑回归加入正则化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归为什么用sigmoid函数"><span class="nav-number">1.5.10.</span> <span class="nav-text">逻辑回归为什么用sigmoid函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#逻辑回归为什么用交叉熵损失，不用MSE；线性回归用MSE"><span class="nav-number">1.5.11.</span> <span class="nav-text">逻辑回归为什么用交叉熵损失，不用MSE；线性回归用MSE</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#支持向量机"><span class="nav-number">1.6.</span> <span class="nav-text">支持向量机</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#支持向量机优缺点"><span class="nav-number">1.6.1.</span> <span class="nav-text">支持向量机优缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM为什么不用梯度下降"><span class="nav-number">1.6.2.</span> <span class="nav-text">SVM为什么不用梯度下降</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#核函数"><span class="nav-number">1.6.3.</span> <span class="nav-text">核函数</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#推导过程"><span class="nav-number">1.6.4.</span> <span class="nav-text">推导过程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SMO优化"><span class="nav-number">1.6.5.</span> <span class="nav-text">SMO优化</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#SVM为什么要用对偶问题来求解"><span class="nav-number">1.6.6.</span> <span class="nav-text">SVM为什么要用对偶问题来求解</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#决策树"><span class="nav-number">1.7.</span> <span class="nav-text">决策树</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ID3决策树"><span class="nav-number">1.7.1.</span> <span class="nav-text">ID3决策树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#C4-5决策树"><span class="nav-number">1.7.2.</span> <span class="nav-text">C4.5决策树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#决策树的剪枝"><span class="nav-number">1.7.3.</span> <span class="nav-text">决策树的剪枝</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#CART树"><span class="nav-number">1.7.4.</span> <span class="nav-text">CART树</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么说bagging降低了方差，boosting降低了偏差"><span class="nav-number">1.7.5.</span> <span class="nav-text">为什么说bagging降低了方差，boosting降低了偏差</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#随机森林和GBDT的区别联系"><span class="nav-number">1.7.6.</span> <span class="nav-text">随机森林和GBDT的区别联系</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#EM算法推导"><span class="nav-number">1.8.</span> <span class="nav-text">EM算法推导</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#主要流程"><span class="nav-number">1.8.1.</span> <span class="nav-text">主要流程</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#完整推导"><span class="nav-number">1.8.2.</span> <span class="nav-text">完整推导</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#收敛性证明"><span class="nav-number">1.8.3.</span> <span class="nav-text">收敛性证明</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HMM隐马尔可夫模型"><span class="nav-number">1.9.</span> <span class="nav-text">HMM隐马尔可夫模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#维特比算法"><span class="nav-number">1.9.1.</span> <span class="nav-text">维特比算法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#HMM-和-CRF-的对比"><span class="nav-number">1.10.</span> <span class="nav-text">HMM 和 CRF 的对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#CRF模型实现细节"><span class="nav-number">1.11.</span> <span class="nav-text">CRF模型实现细节</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#LR和SVM的对比"><span class="nav-number">1.12.</span> <span class="nav-text">LR和SVM的对比</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#机器学习归一化"><span class="nav-number">1.13.</span> <span class="nav-text">机器学习归一化</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chen shuai</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
