<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="en">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="深度学习," />










<meta name="description" content="深度学习梯度下降法梯度是一个函数变化最快的方向，梯度是一个向量，沿着梯度的正方向，可以最快的达到最大值；沿着梯度的反方向，可以最快的达到最小值。  L1正则化不可导怎么处理，使用坐标轴下降法，固定其他参数，保留一个，然后求极值 max操作不可导，可以分段处理 Hinge loss也可以分段处理 SGD、BGD、MBGDSGD就是每次只计算一个样本的梯度来更新参数值 BGD就是使用所有的样本的梯度平">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习常见问题">
<meta property="og:url" content="http://yoursite.com/2020/06/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/index.html">
<meta property="og:site_name" content="小二郎">
<meta property="og:description" content="深度学习梯度下降法梯度是一个函数变化最快的方向，梯度是一个向量，沿着梯度的正方向，可以最快的达到最大值；沿着梯度的反方向，可以最快的达到最小值。  L1正则化不可导怎么处理，使用坐标轴下降法，固定其他参数，保留一个，然后求极值 max操作不可导，可以分段处理 Hinge loss也可以分段处理 SGD、BGD、MBGDSGD就是每次只计算一个样本的梯度来更新参数值 BGD就是使用所有的样本的梯度平">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcr4000619j30bm036glm.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcr4swg8psj30j607wjry.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcr57d8xj8j315c08at9p.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcr59qsgr2j31ns0a2jt9.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcr77g5w6wj31b80jetfk.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcrfit1wecj30u00ufgx2.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gdnkiww5zjj30yr0u0b2b.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcsn3w3peyj31fe0fwjy5.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcsn4ag86mj30p607q78a.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcsn8so0iyj30yz0u0dxa.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcsn9zwmu4j30o004yaar.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcsnb00sslj30w207umy9.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gct9zg72jpj31hu0j8jx6.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gctbic4luzj30rg1947wi.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gctbj90ok3j30ii0bgtja.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gdby46nrlqj310403ojrn.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gctbkuevq0j319i0g2gpa.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1geju9cxen8j30u0156qv7.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gctf7fym54j31140rkk2o.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gctfb9wg08j317e0f2gw2.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjmmbmbvoj30tv0h0ac3.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gd3znzwij3j31a40dcwh4.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gd3zskibs8j313g0dgtbf.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gd3zx4w0e7j30rc06wdgg.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gd3zyjt2r0j31gu04ywg0.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gear2406dej31740iwwgn.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfgmusjjicj30bo04ajri.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gdc6f9dg7ij315o0asabp.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gdmcb61s8dj30u00v41ky.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gdmnkds2cfj30zo0bmq9n.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gdmnkuii10j30kw06675i.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gdrx95mufrj31ds0f4div.jpg">
<meta property="og:image" content="https://tva1.sinaimg.cn/large/007S8ZIlgy1gdrx9i1e5wj30ni03kaa7.jpg">
<meta property="article:published_time" content="2020-06-07T04:50:33.000Z">
<meta property="article:modified_time" content="2020-06-07T04:54:15.605Z">
<meta property="article:author" content="chen shuai">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tva1.sinaimg.cn/large/00831rSTgy1gcr4000619j30bm036glm.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: 'Author'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2020/06/07/深度学习常见问题/"/>





  <title>深度学习常见问题 | 小二郎</title>
  








<meta name="generator" content="Hexo 4.2.1"></head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">小二郎</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Home
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            About
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Tags
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-question-circle"></i> <br />
            
            Archives
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/06/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="chen shuai">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="小二郎">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">深度学习常见问题</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2020-06-07T12:50:33+08:00">
                2020-06-07
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><h3 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h3><p>梯度是一个函数变化最快的方向，梯度是一个向量，沿着梯度的正方向，可以最快的达到最大值；沿着梯度的反方向，可以最快的达到最小值。</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcr4000619j30bm036glm.jpg" alt="image-20200312140512887"></p>
<p>L1正则化不可导怎么处理，使用坐标轴下降法，固定其他参数，保留一个，然后求极值</p>
<p>max操作不可导，可以分段处理</p>
<p>Hinge loss也可以分段处理</p>
<h4 id="SGD、BGD、MBGD"><a href="#SGD、BGD、MBGD" class="headerlink" title="SGD、BGD、MBGD"></a>SGD、BGD、MBGD</h4><p>SGD就是每次只计算一个样本的梯度来更新参数值</p>
<p>BGD就是使用所有的样本的梯度平均值来更新参数值</p>
<p>MBGD就是采用部分样本的梯度平均值来更新参数值</p>
<h4 id="梯度下降的优化版本"><a href="#梯度下降的优化版本" class="headerlink" title="梯度下降的优化版本"></a>梯度下降的优化版本</h4><p>我们知道如果每次都是对一个batch计算梯度来更新，很容易造成抖动的情况，所以有没有办法能够稳定梯度，也就是不仅考虑当前的梯度还要考虑之前的梯度</p>
<h5 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h5><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcr4swg8psj30j607wjry.jpg" alt="image-20200312143259002" style="zoom:33%;" />

<p>带冲量的梯度下降。它的动机就是说保存历史累计梯度，其实就是对历史累计梯度做了一个加权和，称作冲量。然后计算当前时刻的梯度，如果与冲量方向一致，那么就是叠加进去，如果相反，冲量也会减少。这种优化器的好处也显而易见，考虑了历史累计梯度，可以避免模型在某个时刻过于震荡，从而加速模型的训练。</p>
<h5 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h5><p>是RMSProp前一个版本，让学习率衰减，让学习率除以历史累积的梯度平方和在开根</p>
<h5 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h5><p>梯度的问题解决了，那么学习率是不是有更好的办法让它能够自适应的改变呢</p>
<p>是Adagrad改进版，adagrad训练到一定程度，学习率太小了，不太靠谱。所以采用指数衰减的策略，只考虑最近的梯度</p>
<p>可以看到，s是关于梯度的，如果梯度太大，那么也就是s很大，用alpha去除，那么alpha就会降低，相当于防止过度抖动</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcr57d8xj8j315c08at9p.jpg" alt="image-20200312144653595"></p>
<h5 id="Adam算法"><a href="#Adam算法" class="headerlink" title="Adam算法"></a>Adam算法</h5><p>Adam算法就是取两者的优点，既平滑梯度，也平滑学习率</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcr59qsgr2j31ns0a2jt9.jpg" alt="image-20200312144910210"></p>
<h3 id="反向传播"><a href="#反向传播" class="headerlink" title="反向传播"></a>反向传播</h3><p>反向传播就是利用链式求导法则一步步后推罢了</p>
<h3 id="卷积神经网络"><a href="#卷积神经网络" class="headerlink" title="卷积神经网络"></a>卷积神经网络</h3><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcr77g5w6wj31b80jetfk.jpg" alt="image-20200312155609581"></p>
<h4 id="反向传播-1"><a href="#反向传播-1" class="headerlink" title="反向传播"></a>反向传播</h4><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcrfit1wecj30u00ufgx2.jpg" alt="image-20200312204349392"></p>
<h4 id="pooling层反向传播"><a href="#pooling层反向传播" class="headerlink" title="pooling层反向传播"></a>pooling层反向传播</h4><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdnkiww5zjj30yr0u0b2b.jpg" alt="image-20200409155445667"></p>
<h4 id="1-1卷积有什么用"><a href="#1-1卷积有什么用" class="headerlink" title="1*1卷积有什么用"></a>1*1卷积有什么用</h4><p>一方面呢实现了对数据通道数的一个降纬或升纬。另一方面呢可以对多个通道feature map的信息做一个融合。实现了全连接的效果</p>
<h4 id="全连接和CNN区别"><a href="#全连接和CNN区别" class="headerlink" title="全连接和CNN区别"></a>全连接和CNN区别</h4><p>全连接的话是对所有的输入元素做一个加权和，有种采用大的卷积核的感觉</p>
<p>而CNN是采用小的卷积核，然后权值共享，通过滑动卷积核来实现对整个feature map的特征提取</p>
<h3 id="循环神经网络"><a href="#循环神经网络" class="headerlink" title="循环神经网络"></a>循环神经网络</h3><h4 id="RNN展开图"><a href="#RNN展开图" class="headerlink" title="RNN展开图"></a>RNN展开图</h4><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcsn3w3peyj31fe0fwjy5.jpg" alt="image-20200313215148276"></p>
<h4 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h4><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcsn4ag86mj30p607q78a.jpg" alt="image-20200313215214682" style="zoom:33%;" />

<h4 id="反向传播BPTT算法"><a href="#反向传播BPTT算法" class="headerlink" title="反向传播BPTT算法"></a>反向传播BPTT算法</h4><p>BPTT其实就是标准的梯度反向传播算法在RNN中的应用罢了，只是名字搞得花里胡哨，因为S状态是传递的，而且与W、U有关，所以导致求导的时候也是要按时间步展开的，具体流程流程如下</p>
<p>设E3为第三个时间步的损失，下面开始求偏导</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcsn8so0iyj30yz0u0dxa.jpg" alt="image-20200313215634543"></p>
<h4 id="梯度消失、爆炸原因"><a href="#梯度消失、爆炸原因" class="headerlink" title="梯度消失、爆炸原因"></a>梯度消失、爆炸原因</h4><p>上面这个公式还要在转换一下形式</p>
<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcsn9zwmu4j30o004yaar.jpg" alt="image-20200313215743977" style="zoom:33%;" />

<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gcsnb00sslj30w207umy9.jpg" alt="image-20200313215842073" style="zoom:50%;" />

<p>可以看出这里对tanh函数的导数有一个长范围的连乘，但是tanh导数的值在0-1之间，且大多数时候都是小于1的，所以就很容易造成梯度消失。同理如果那个矩阵中有值比较大，则连乘后就容易造成梯度爆炸。</p>
<h4 id="LSTM网络"><a href="#LSTM网络" class="headerlink" title="LSTM网络"></a>LSTM网络</h4><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gct9zg72jpj31hu0j8jx6.jpg" alt="image-20200314110321695"></p>
<p>完整推导</p>
<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gctbic4luzj30rg1947wi.jpg" alt="image-20200314115606081" style="zoom:50%;" />

<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gctbj90ok3j30ii0bgtja.jpg" alt="image-20200314115659042" style="zoom: 50%;" />

<h5 id="参数量计算"><a href="#参数量计算" class="headerlink" title="参数量计算"></a>参数量计算</h5><p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdby46nrlqj310403ojrn.jpg" alt="image-20200330143826086"></p>
<p>三个门的计算就是3个，还要在加一个c_t的计算</p>
<h4 id="GRU网络"><a href="#GRU网络" class="headerlink" title="GRU网络"></a>GRU网络</h4><p>GRU就是将LSTM的三个门变成了两个门，将LSTM的输入门和遗忘门合并为update门，还有一个reset门。由于在LSTM中隐状态ht也是从Ct输出转化得到的，那么干脆把Ct当作ht，然后取消了ht。</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gctbkuevq0j319i0g2gpa.jpg" alt="image-20200314115831618"></p>
<h4 id="RNN、LSTM、GRU对比"><a href="#RNN、LSTM、GRU对比" class="headerlink" title="RNN、LSTM、GRU对比"></a>RNN、LSTM、GRU对比</h4><p>首先这三种模型都是循环神经网络，LSTM、GRU是RNN的变种，适用于序列建模</p>
<p>与普通的全联接模型相比呢，循环神经网络的特点就是当前的输出不仅与当前的输入x有关，还和上一个时刻的隐状态h有关。</p>
<p>但是RNN在实际应用中有一个问题就是容易出现梯度消失或者爆炸，这个怎么理解呢，可以误差反向传播公示推倒一遍</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1geju9cxen8j30u0156qv7.jpg" alt="image-20200507134940225"></p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>dropout的核心思想就是通过使得部分神经元的值为0，从而相当于每次只是训练一个子网络，那么当测试的时候就是很多个子网络ensemble的结果，有点像是bagging的思想</p>
<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gctf7fym54j31140rkk2o.jpg" alt="image-20200314140402032" style="zoom:33%;" />

<p>公式可以看出就是一一个概率值决定哪些神经元为0，不过有一点要注意，每次dropout后，要对该层的输出进行rescale，因为你丢掉了部分值，为了测试的时候不影响，所以你得除以kepp_prob p，这样的话保证这层的值是差不多的。</p>
<h4 id="dropout为什么可以缓解过拟合"><a href="#dropout为什么可以缓解过拟合" class="headerlink" title="dropout为什么可以缓解过拟合"></a>dropout为什么可以缓解过拟合</h4><ol>
<li>通过每次随机使得一部分神经元失活，相当于给模型添加噪声，强制模型学习一些更鲁棒的特征</li>
<li>另一方面使得网络结构更简单些，模型简单从而使得解空间变小，也可以起到缓解过拟合的目的</li>
<li>最终输出相当于做了一个bagging</li>
</ol>
<h4 id="keras中dropout代码实现"><a href="#keras中dropout代码实现" class="headerlink" title="keras中dropout代码实现"></a>keras中dropout代码实现</h4><p>可以看出keras中对dropout就是每次dropout后进行一个rescale。</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gctfb9wg08j317e0f2gw2.jpg" alt="image-20200314140743638"></p>
<h4 id="dropout缺点"><a href="#dropout缺点" class="headerlink" title="dropout缺点"></a>dropout缺点</h4><ol>
<li>因为每次都是相当于训练一个子网络，所以收敛会变慢，训练时间会变长</li>
<li>代价函数没法被准确定义，因为每次都随机置0一些神经元</li>
</ol>
<h4 id="dropout反向传播"><a href="#dropout反向传播" class="headerlink" title="dropout反向传播"></a>dropout反向传播</h4><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfjmmbmbvoj30tv0h0ac3.jpg" alt="image-20200607124549136"></p>
<h3 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h3><h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><p>作者使用三角函数编码和学习编码效果差不多，所以为了简单起见，直接使用了三角函数编码</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd3znzwij3j31a40dcwh4.jpg" alt="image-20200323172721437"></p>
<h4 id="位置编码为何有效"><a href="#位置编码为何有效" class="headerlink" title="位置编码为何有效"></a>位置编码为何有效</h4><p>论文里面说法是sin cos函数可以引入相关位置信息，就是pos+k的位置编码可以通过pos的编码加一个线性函数表示出来</p>
<h4 id="MultiHead-Attention"><a href="#MultiHead-Attention" class="headerlink" title="MultiHead Attention"></a>MultiHead Attention</h4><p>首先要意识到multihead attention其实就是self-attention，只不过是切分成几个头然后在拼接的</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd3zskibs8j313g0dgtbf.jpg" alt="image-20200323173148012"></p>
<p>然后这里的self-attention计算方式 其实采用的是scaled dot-production</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd3zx4w0e7j30rc06wdgg.jpg" alt="image-20200323173611197"></p>
<p>为什么使用scaled呢</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gd3zyjt2r0j31gu04ywg0.jpg" alt="image-20200323173732423"></p>
<h4 id="为什么使用MultiHead-Attention"><a href="#为什么使用MultiHead-Attention" class="headerlink" title="为什么使用MultiHead Attention"></a>为什么使用MultiHead Attention</h4><p>作者的解释是不同的头可以关注不同的表示空间，这样的话多个头就可以关注多个不同的特征，最后在拼接，这样的话无疑能捕捉到更多的句子信息</p>
<h4 id="为什么使用layer-normalization而不是batch-normalization"><a href="#为什么使用layer-normalization而不是batch-normalization" class="headerlink" title="为什么使用layer_normalization而不是batch_normalization"></a>为什么使用layer_normalization而不是batch_normalization</h4><p>Batch_normalization简介：</p>
<p>BN主要是针对batch这一维度上的值做一个规范化，</p>
<p>从两个角度来思考BN：</p>
<ol>
<li>ICS 内部分布偏移现象，原论文的解释，针对某一层，随着参数的改变，输出的分布也是不断改变的，模型学习的就是数据的分布，这样变化的太频繁不利于收敛，所以作归一化，保证输出稳定。另外为了能还原分布，还额外引入变量，模型可以自己学习。</li>
<li>sigmoid做激活函数的话，激活函数的敏感区域比较小，如果上一层输出值比较分散，会造成梯度较小，所以用BN将btach这个维度的分布转换成一个正态分布。但是为了转换后的分布还是要和以前的分布能够还原，所以还要再做个转换</li>
</ol>
<p>Layer_normalization的优势：</p>
<ul>
<li>BN是对一个batch的统计量，所以batch过小，那么得到的统计值不准确。而LN是针对一个样本去统计，所以不存在这个问题</li>
<li>BN应用在RNN中时，由于有多个time_step，所以要每个time_step都要统计，这就需要额外的开销。LN没有这个问题</li>
<li>一篇论文的说法，在transformer训练时，batch维度上的值波动比较大，如果使用BN会导致模型的不稳定</li>
</ul>
<p>BN和LN推理阶段怎么处理？</p>
<p>LN好理解，是针对样本的，所以测试的时候直接算就完事了</p>
<p>BN麻烦点，需要用到历史累计的方差，均值的滑动平均</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gear2406dej31740iwwgn.jpg" alt="image-20200429170903485"></p>
<h4 id="self-attention和feed-forward层作用"><a href="#self-attention和feed-forward层作用" class="headerlink" title="self-attention和feed_forward层作用"></a>self-attention和feed_forward层作用</h4><p>self-attention就是自身的每个词去和其他词做一个attention计算，然后转化为其他所有词的一个加权和，这样的话每个词就和其他词建立了联系，相当于学习到了一个上下文特征。multi-head attention就是不同的头去捕捉不同的特征，这样提取特征的能力就更强。</p>
<p>Feed-forward的作用我自己的理解就是multi-head attention后，模型需要对层的输出做一个信息整合，所以用两层全联接加非线性激活函数来做一个信息融合</p>
<h4 id="Encoder-decoder间的attention"><a href="#Encoder-decoder间的attention" class="headerlink" title="Encoder decoder间的attention"></a>Encoder decoder间的attention</h4><p>K,V来自于编码器的输出，Q来自于上一时刻解码器的输出</p>
<h4 id="为什么PE相加不是concate"><a href="#为什么PE相加不是concate" class="headerlink" title="为什么PE相加不是concate"></a>为什么PE相加不是concate</h4><p>我的想法就是这层组合的embedding进入下一层网络时还是要经过全连接进行转换的，这里如果拼接的话那么还是相当于加权和的形式，和直接加和在过全连接差不多。所以说两种方式应该效果上是差不多的。</p>
<h3 id="BERT"><a href="#BERT" class="headerlink" title="BERT"></a>BERT</h3><p>BERT是一个双向语言模型，主要包含两个阶段，分别是预训练语言模型和下游任务fine-tune</p>
<p>预训练语言模型阶段主要是采用遮蔽语言模型的训练方法，也就是将一句话15%的token置换成[MASK]，但担心下游任务不会有[MASK]标志符，所以又对这些[MASK]进一步处理。最后模型的目标就是通过上下文去预测被mask掉的这个token。另外为了捕捉句子间的关系，还设计了一个NSP任务，连续的两句话构成正例，否则构成负例。</p>
<p>针对下游任务的fine-tune呢，就是将最后的输出层第一个token的表示拿出来作为输入句子的表示，后接分类层做文本分类、情感分析等任务</p>
<h4 id="为什么采用warm-up"><a href="#为什么采用warm-up" class="headerlink" title="为什么采用warm-up"></a>为什么采用warm-up</h4><p>直观上的理解：一开始loss比较大，如果学习率过大的话，会造成模型参数的巨大抖动，所以先慢慢来，逐步提高</p>
<p>这样就是有助于保证模型的稳定</p>
<p>学习率后期的decay也是怕学习率过大导致震荡，所以逐步减小</p>
<h4 id="位置编码-1"><a href="#位置编码-1" class="headerlink" title="位置编码"></a>位置编码</h4><p>BERT采用学习的位置编码，而不是transformer所采用的三角函数</p>
<h4 id="BERT之后和之前的工作"><a href="#BERT之后和之前的工作" class="headerlink" title="BERT之后和之前的工作"></a>BERT之后和之前的工作</h4><p>BERT之后的工作主要集中在两方面，第一种是使BERT更轻量化，第二种是更改或者优化预训练任务使得性能更好</p>
<h5 id="轻量化"><a href="#轻量化" class="headerlink" title="轻量化"></a>轻量化</h5><p>典型工作，ALBERT</p>
<p>主要集中在以下几点：1. Word embedding 分解，降低word embedding，加大hidden size；vocab*e , 减小e. 2. 跨层参数共享，共享feed_forward网络，共享attention那一层参数 3. 取消NSP任务，提出句子顺序是否被调换任务，即SOP</p>
<h5 id="更改预训练任务"><a href="#更改预训练任务" class="headerlink" title="更改预训练任务"></a>更改预训练任务</h5><p>Roberta就是加大数据量，取消NSP任务，更大的batch_size</p>
<p>ERNIE就是更改mask策略，使用短语mask和实体mask</p>
<p>ELECTRA就是预测一个token是否被替换</p>
<p>XLNET将自回归语言模型和双向语言模型结合，具体方式就是排列输入的顺序，使得看到的上文信息有下文的信息</p>
<h4 id="我认为未来的优化方向"><a href="#我认为未来的优化方向" class="headerlink" title="我认为未来的优化方向"></a>我认为未来的优化方向</h4><p>分为三个阶段，第一个阶段就是大规模同样语料的预训练，第二阶段就是在特定领域引入知识表示，最后一阶段才是在该领域的任务上进行fine-tune</p>
<h4 id="ELMO-GPT-BERT对比"><a href="#ELMO-GPT-BERT对比" class="headerlink" title="ELMO GPT BERT对比"></a>ELMO GPT BERT对比</h4><p>ELMO主要是三层，第一层也就是输入层，采用就是word embedding作为输入，然后另外两层是双向lstm，分别是做一个前向语言模型，和后向语言模型，然后来训练语言模型。最后针对下游任务的时候可以将三层embedding加权叠加，这样的话其实就相当于做了一个动态的word embedding</p>
<p>GPT是单向语言模型，他是使用transformer来作为编码器，根据之前的单词来预测下一个</p>
<p>BERT改进了gpt使用了双向语言模型的思想，使用上下文来预测当前的单词。</p>
<h3 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h3><p>word2vec是一种将词映射到一个低纬稠密向量空间的方法，也就是词嵌入。它主要包含两种，CBOW和skip-gram，CBOW就是扣掉某词，用上下文来预测它。skip-gram相反。</p>
<p>word2vec有两种优化方式，第一种叫层次softmax，第二种是负采样。</p>
<h4 id="层次softmax"><a href="#层次softmax" class="headerlink" title="层次softmax"></a>层次softmax</h4><p>首先根据训练语料的词典构建霍夫曼树，这样的话高频词就处在较前面的位置。霍夫曼树的根节点是隐藏层的输出向量，然后使用逻辑回归计算概率。每个非叶子节点都是一个向量。就是本来的隐藏层到输出层这个矩阵参数变成了非叶子结点。为什么使用层次softmax呢，因为普通的softmax要对整个词典去计算概率，然后取最大的那个词，这样太耗费资源。而使用霍夫曼树，因为我训练是知道他的target word的，所以我可以快速找到这个词，也就是霍夫曼树的某个节点，然霍夫曼树每条路径其实都是概率的连乘，这样我们就可以最大化那个目标词的概率就好了。</p>
<h5 id="霍夫曼树的构建"><a href="#霍夫曼树的构建" class="headerlink" title="霍夫曼树的构建"></a>霍夫曼树的构建</h5><p>就是每次找两个最小的结点，然后构造根节点是两个结点之和。再把这个根节点放入原数据，在找两个最小的结点。</p>
<h4 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h4><p>负采样的思想就是我每次都更新所有词的向量，这样也太耗时了。所以我能不能每次只更新部分，然后目标词是正样本，这个肯定要更新。而其他的词呢就是负样本，我可以以一个概率去选择更新哪些词，这个概率可以与词频成正比。</p>
<h5 id="负采样比较容易采样到高频词解决办法"><a href="#负采样比较容易采样到高频词解决办法" class="headerlink" title="负采样比较容易采样到高频词解决办法"></a>负采样比较容易采样到高频词解决办法</h5><p>对词频开3/4幂，这样的话会对高频词影响较大，把它的频率拉下来，提高一下低频词被采样到的概率。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfgmusjjicj30bo04ajri.jpg" alt="image-20200604223721015"></p>
<h4 id="word2vec的gensim库训练"><a href="#word2vec的gensim库训练" class="headerlink" title="word2vec的gensim库训练"></a>word2vec的gensim库训练</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gensim.models.word2vec.Word2Vec(sentences=<span class="literal">None</span>, size=<span class="number">100</span>, alpha=<span class="number">0.025</span>, window=<span class="number">5</span>, min_count=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<h4 id="其他的文本表示方法"><a href="#其他的文本表示方法" class="headerlink" title="其他的文本表示方法"></a>其他的文本表示方法</h4><p>主要分为两大类，离散表示，分布式表示</p>
<p>分布式表示就是word2vec,glove这种</p>
<ul>
<li>glove词向量就是考虑的共现信息，比如三个词，i,j,k， p(i,k)很小，但是比率比较大，通过比率也可以看出哪两个词更相关，所以这时候模型就可以对i,j,k这三个词进行建模，拟合的目标就是他们之间的比值</li>
</ul>
<p>离散表示就是词袋模型，tf-idf表示，n-gram表示</p>
<ul>
<li><p>词袋模型，没有词的顺序，构建一个词典，出现了某个词，就在那个index置1</p>
</li>
<li><p>Tf-idf，还是构建一个词典纬度的向量，tf就是该词在该文档中的频率，idf就是逆文档频率</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdc6f9dg7ij315o0asabp.jpg" alt="image-20200330192554917"></p>
</li>
<li><p>N-gram 就是每n个连续的词也加入到词典，在去进行下面同样的操作</p>
</li>
</ul>
<h3 id="为什么神经网络初始权重不能为0"><a href="#为什么神经网络初始权重不能为0" class="headerlink" title="为什么神经网络初始权重不能为0"></a>为什么神经网络初始权重不能为0</h3><p>如果为0，会导致后续隐藏层的所有神经元的值都是一样的，然后梯度下降的时候，梯度改变值也是一样的，那么就会造成神经元不能学习不同的特征。</p>
<h3 id="过拟合及优化专项"><a href="#过拟合及优化专项" class="headerlink" title="过拟合及优化专项"></a>过拟合及优化专项</h3><h4 id="什么是过拟合"><a href="#什么是过拟合" class="headerlink" title="什么是过拟合"></a>什么是过拟合</h4><p>当模型在训练集上面性能越来越高，但在测试集上不增反降，这种现象就是过拟合</p>
<h4 id="过拟合的原因"><a href="#过拟合的原因" class="headerlink" title="过拟合的原因"></a>过拟合的原因</h4><p>模型的解空间过大，而从这有限的数据或者特征来说，无法挑选出一个比较好的模型。所以从这么大的解空间里硬要挑一个模型，是很有可能在训练上表现好但是测试集表现差的。</p>
<h4 id="如何改善"><a href="#如何改善" class="headerlink" title="如何改善"></a>如何改善</h4><p>最简单的操作，增加数据量</p>
<p>模型角度呢，其实都可以看作是缩小解空间</p>
<p>为什么L1，L2相当于缩小了解空间呢？</p>
<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdmcb61s8dj30u00v41ky.jpg" alt="image-20200408142459145" style="zoom:50%;" />

<p>dropout和batch_normalization查看上文</p>
<h3 id="Attention总结"><a href="#Attention总结" class="headerlink" title="Attention总结"></a>Attention总结</h3><p>这个网上资料有分类成soft attention(就是常规attention)和hard attention(不可导)，根据计算区域分为global attention (对所有的k计算attention)和 local attention(窗口内部分k做attention)。我自己还是比较倾向于分成两个大类，第一个就是encoder和decoder之间的这种attention，第二类就是自己和自己的attention</p>
<p>attention一开始在NLP里的应用是seq2seq这种结构。原始的做法一般就是encoder把输入编码成一个固定的context，然后交由解码器去解码。但是这种显然是不合理的，解码的每一个状态应该与输入端的不同状态联系是不一样的，所以呢我在解码端就去用隐状态和输入端的每一个状态做一个相似度计算，这里的相似度计算可以用乘性的attention也可以用加性的attention，比如我们这里用点积，计算完相似度后，我们对这个向量进行softmax，相当于是一个权重，然后对输入端计算一个加权和，那么这个context就和解码端的联系就比较紧密了，解码端不同的状态可以关注到不同的输入端信息。</p>
<p>后来一个就是出来了self-attention，这个就是顾名思义，就是自己和自己做一个attention。比如一个句子，将每个token去和其他的token做一个attention,然后加权求和，这样的话，每个token都相当于与其他的token都形成了联系。这比rnn好的地方就是rnn长距离依赖不行，但是self-attention无视距离关系，所以没有这种缺点。</p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdmnkds2cfj30zo0bmq9n.jpg" alt="image-20200408205427460"></p>
<p><img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdmnkuii10j30kw06675i.jpg" alt="image-20200408205457419"></p>
<h3 id="为什么分类模型用交叉熵损失，回归模型用MSE损失"><a href="#为什么分类模型用交叉熵损失，回归模型用MSE损失" class="headerlink" title="为什么分类模型用交叉熵损失，回归模型用MSE损失"></a>为什么分类模型用交叉熵损失，回归模型用MSE损失</h3><p>首先要知道，分类问题一般是用softmax做激活函数，而这里使用交叉熵作为损失函数一般是要比MSE更能收敛到一个好的极值点的。</p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gdrx95mufrj31ds0f4div.jpg" alt="image-20200413101722976"></p>
<p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gdrx9i1e5wj30ni03kaa7.jpg" alt="image-20200413101745367"></p>
<h3 id="什么是梯度下降"><a href="#什么是梯度下降" class="headerlink" title="什么是梯度下降"></a>什么是梯度下降</h3><p>梯度是一个函数变化最快的方向，而负梯度就是一个函数下降最快的方向，梯度下降法就是每次计算当前函数的负梯度，然后沿着这个方向走，来达到不断减小目标函数的目的</p>
<h3 id="什么是反向传播"><a href="#什么是反向传播" class="headerlink" title="什么是反向传播"></a>什么是反向传播</h3><p>对于一个有多个隐藏层的神经网络，输出层的误差信息是无法直接反映到中间层的权重上的，反向传播其实就是利用链式求导法则将输出层的误差传播到中间层的权重上，让中间层的权重得以更新参数</p>
<h3 id="梯度消失-爆炸原因"><a href="#梯度消失-爆炸原因" class="headerlink" title="梯度消失/爆炸原因"></a>梯度消失/爆炸原因</h3><p>首先梯度消失/爆炸是网络的权重和激活函数一起作用的结果，当梯度反向传播的时候，会有激活函数的导数连乘和权重的连乘，比如说这里使用sigmoid函数的话，因为sigmoid饱和区域也就是梯度接近0的区域比较大，所以连乘后容易造成梯度消失。而如果权重比较大的话，就容易造成梯度爆炸。</p>
<p>梯度爆炸相对来说容易解决，可以使用梯度裁剪</p>
<p>梯度消失比较难搞，可以换激活函数，使用BN层等方式</p>
<h3 id="激活函数总结"><a href="#激活函数总结" class="headerlink" title="激活函数总结"></a>激活函数总结</h3><p>首先理解激活函数的作用，为模型引入非线性表示，如果没有非线性表示的话，无论多么深的网络最后其实都可以写成一个线性组合，就是乘一个权重加上偏置。</p>
<p>那么sigmoid,tanh激活函数缺点是什么呢？就是敏感区域也就是梯度值比较大的区域比较小，网络比较深的时候很容易造成梯度消失。</p>
<p>relu只是可以减缓这种现象，虽然在负数那一侧也会梯度为0，但是比较少。更多的还是在正数一侧。relu还有一个好处就是可以使得网络具有稀疏性，稀疏性为什么好，我的看法是这种东西跟dropout差不多，使得模型更关注典型的特征，摒弃掉那些无效的特征。另外relu可以加速网络的训练，因为他的导数相比sigmoid来说容易计算很多。</p>
<h3 id="神经网络偏置的作用"><a href="#神经网络偏置的作用" class="headerlink" title="神经网络偏置的作用"></a>神经网络偏置的作用</h3><p>不加偏置，必然经过原点。加了偏置其实就是增强了表达能力</p>

      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2020/06/07/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" rel="next" title="机器学习常见问题">
                <i class="fa fa-chevron-left"></i> 机器学习常见问题
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">chen shuai</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">4</span>
                  <span class="site-state-item-name">posts</span>
                </a>
              </div>
            

            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">tags</span>
                </a>
              </div>
            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#深度学习"><span class="nav-number">1.</span> <span class="nav-text">深度学习</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度下降法"><span class="nav-number">1.1.</span> <span class="nav-text">梯度下降法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#SGD、BGD、MBGD"><span class="nav-number">1.1.1.</span> <span class="nav-text">SGD、BGD、MBGD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度下降的优化版本"><span class="nav-number">1.1.2.</span> <span class="nav-text">梯度下降的优化版本</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#Momentum"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Adagrad"><span class="nav-number">1.1.2.2.</span> <span class="nav-text">Adagrad</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#RMSProp"><span class="nav-number">1.1.2.3.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Adam算法"><span class="nav-number">1.1.2.4.</span> <span class="nav-text">Adam算法</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#反向传播"><span class="nav-number">1.2.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#卷积神经网络"><span class="nav-number">1.3.</span> <span class="nav-text">卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#反向传播-1"><span class="nav-number">1.3.1.</span> <span class="nav-text">反向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#pooling层反向传播"><span class="nav-number">1.3.2.</span> <span class="nav-text">pooling层反向传播</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#1-1卷积有什么用"><span class="nav-number">1.3.3.</span> <span class="nav-text">1*1卷积有什么用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#全连接和CNN区别"><span class="nav-number">1.3.4.</span> <span class="nav-text">全连接和CNN区别</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#循环神经网络"><span class="nav-number">1.4.</span> <span class="nav-text">循环神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#RNN展开图"><span class="nav-number">1.4.1.</span> <span class="nav-text">RNN展开图</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#前向算法"><span class="nav-number">1.4.2.</span> <span class="nav-text">前向算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#反向传播BPTT算法"><span class="nav-number">1.4.3.</span> <span class="nav-text">反向传播BPTT算法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#梯度消失、爆炸原因"><span class="nav-number">1.4.4.</span> <span class="nav-text">梯度消失、爆炸原因</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#LSTM网络"><span class="nav-number">1.4.5.</span> <span class="nav-text">LSTM网络</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#参数量计算"><span class="nav-number">1.4.5.1.</span> <span class="nav-text">参数量计算</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#GRU网络"><span class="nav-number">1.4.6.</span> <span class="nav-text">GRU网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#RNN、LSTM、GRU对比"><span class="nav-number">1.4.7.</span> <span class="nav-text">RNN、LSTM、GRU对比</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Dropout"><span class="nav-number">1.5.</span> <span class="nav-text">Dropout</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#dropout为什么可以缓解过拟合"><span class="nav-number">1.5.1.</span> <span class="nav-text">dropout为什么可以缓解过拟合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#keras中dropout代码实现"><span class="nav-number">1.5.2.</span> <span class="nav-text">keras中dropout代码实现</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dropout缺点"><span class="nav-number">1.5.3.</span> <span class="nav-text">dropout缺点</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#dropout反向传播"><span class="nav-number">1.5.4.</span> <span class="nav-text">dropout反向传播</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Transformer"><span class="nav-number">1.6.</span> <span class="nav-text">Transformer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#位置编码"><span class="nav-number">1.6.1.</span> <span class="nav-text">位置编码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#位置编码为何有效"><span class="nav-number">1.6.2.</span> <span class="nav-text">位置编码为何有效</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#MultiHead-Attention"><span class="nav-number">1.6.3.</span> <span class="nav-text">MultiHead Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么使用MultiHead-Attention"><span class="nav-number">1.6.4.</span> <span class="nav-text">为什么使用MultiHead Attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么使用layer-normalization而不是batch-normalization"><span class="nav-number">1.6.5.</span> <span class="nav-text">为什么使用layer_normalization而不是batch_normalization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#self-attention和feed-forward层作用"><span class="nav-number">1.6.6.</span> <span class="nav-text">self-attention和feed_forward层作用</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Encoder-decoder间的attention"><span class="nav-number">1.6.7.</span> <span class="nav-text">Encoder decoder间的attention</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么PE相加不是concate"><span class="nav-number">1.6.8.</span> <span class="nav-text">为什么PE相加不是concate</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#BERT"><span class="nav-number">1.7.</span> <span class="nav-text">BERT</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#为什么采用warm-up"><span class="nav-number">1.7.1.</span> <span class="nav-text">为什么采用warm-up</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#位置编码-1"><span class="nav-number">1.7.2.</span> <span class="nav-text">位置编码</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#BERT之后和之前的工作"><span class="nav-number">1.7.3.</span> <span class="nav-text">BERT之后和之前的工作</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#轻量化"><span class="nav-number">1.7.3.1.</span> <span class="nav-text">轻量化</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#更改预训练任务"><span class="nav-number">1.7.3.2.</span> <span class="nav-text">更改预训练任务</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#我认为未来的优化方向"><span class="nav-number">1.7.4.</span> <span class="nav-text">我认为未来的优化方向</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ELMO-GPT-BERT对比"><span class="nav-number">1.7.5.</span> <span class="nav-text">ELMO GPT BERT对比</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#word2vec"><span class="nav-number">1.8.</span> <span class="nav-text">word2vec</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#层次softmax"><span class="nav-number">1.8.1.</span> <span class="nav-text">层次softmax</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#霍夫曼树的构建"><span class="nav-number">1.8.1.1.</span> <span class="nav-text">霍夫曼树的构建</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#负采样"><span class="nav-number">1.8.2.</span> <span class="nav-text">负采样</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#负采样比较容易采样到高频词解决办法"><span class="nav-number">1.8.2.1.</span> <span class="nav-text">负采样比较容易采样到高频词解决办法</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#word2vec的gensim库训练"><span class="nav-number">1.8.3.</span> <span class="nav-text">word2vec的gensim库训练</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#其他的文本表示方法"><span class="nav-number">1.8.4.</span> <span class="nav-text">其他的文本表示方法</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么神经网络初始权重不能为0"><span class="nav-number">1.9.</span> <span class="nav-text">为什么神经网络初始权重不能为0</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#过拟合及优化专项"><span class="nav-number">1.10.</span> <span class="nav-text">过拟合及优化专项</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#什么是过拟合"><span class="nav-number">1.10.1.</span> <span class="nav-text">什么是过拟合</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#过拟合的原因"><span class="nav-number">1.10.2.</span> <span class="nav-text">过拟合的原因</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#如何改善"><span class="nav-number">1.10.3.</span> <span class="nav-text">如何改善</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Attention总结"><span class="nav-number">1.11.</span> <span class="nav-text">Attention总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#为什么分类模型用交叉熵损失，回归模型用MSE损失"><span class="nav-number">1.12.</span> <span class="nav-text">为什么分类模型用交叉熵损失，回归模型用MSE损失</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是梯度下降"><span class="nav-number">1.13.</span> <span class="nav-text">什么是梯度下降</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#什么是反向传播"><span class="nav-number">1.14.</span> <span class="nav-text">什么是反向传播</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#梯度消失-爆炸原因"><span class="nav-number">1.15.</span> <span class="nav-text">梯度消失&#x2F;爆炸原因</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#激活函数总结"><span class="nav-number">1.16.</span> <span class="nav-text">激活函数总结</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#神经网络偏置的作用"><span class="nav-number">1.17.</span> <span class="nav-text">神经网络偏置的作用</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">chen shuai</span>

  
</div>


  <div class="powered-by">Powered by <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
